
# 🛡️ Graph-based Adversarial Robustness Testing for GNNs

This notebook explores adversarial attacks on Graph Neural Networks (GNNs) trained to detect speculative execution vulnerabilities using Control Flow Graphs (CFGs).

---

## 📦 Step 1: Setup

Install required packages.

# !pip install torch torch_geometric networkx sklearn numpy matplotlib

import torch
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from torch_geometric.data import Data
from sklearn.metrics import classification_report

## 🧪 Step 2: Define a Sample Graph and GNN Classifier

We'll reuse the same structure as before but focus on testing robustness with perturbed graphs.

def create_mock_cfg_vulnerable():
    G = nx.DiGraph()
    G.add_node(0, label='load r1')
    G.add_node(1, label='if (secret)')
    G.add_node(2, label='access array[r1]')
    G.add_edges_from([(0,1), (1,2)])
    return G

def create_mock_cfg_safe():
    G = nx.DiGraph()
    G.add_node(0, label='load r1')
    G.add_node(1, label='mask r1')
    G.add_node(2, label='access array[r1]')
    G.add_edges_from([(0,1), (1,2)])
    return G

def graph_to_data(graph, label):
    node_features = []
    for _, data in graph.nodes(data=True):
        text = data.get('label', '')
        features = [
            int('if' in text),
            int('mask' in text),
            int('access' in text),
            len(text)
        ]
        node_features.append(features)

    x = torch.tensor(node_features, dtype=torch.float)
    edge_index = torch.tensor(list(graph.edges), dtype=torch.long).t().contiguous()
    y = torch.tensor([label], dtype=torch.long)

    return Data(x=x, edge_index=edge_index, y=y)

from torch_geometric.nn import GCNConv, global_mean_pool

class GCN(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.lin = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index).relu()
        x = global_mean_pool(x, batch)
        return self.lin(x)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN(4, 16, 2).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()

data_list = [
    graph_to_data(create_mock_cfg_safe(), 0),
    graph_to_data(create_mock_cfg_vulnerable(), 1)
]

def train(data_list):
    model.train()
    total_loss = 0
    for data in data_list:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long).to(device))
        loss = criterion(out, data.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(data_list)

for epoch in range(1, 21):
    loss = train(data_list)
    print(f"Epoch {epoch}, Loss: {loss:.4f}")

## 🧨 Step 3: Apply a Simple Adversarial Perturbation

def perturb_graph(graph):
    G = graph.copy()
    if G.number_of_nodes() >= 3:
        G.add_edge(0, 2)  # Skip control node
    return G

# Evaluate original vs adversarial
model.eval()
original = graph_to_data(create_mock_cfg_vulnerable(), 1).to(device)
perturbed = graph_to_data(perturb_graph(create_mock_cfg_vulnerable()), 1).to(device)

def predict(data):
    with torch.no_grad():
        out = model(data.x, data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long).to(device))
        return torch.argmax(out).item()

print("Original prediction (vulnerable):", predict(original))
print("Perturbed prediction:", predict(perturbed))
