
# ðŸ”— GNN-based Speculative Execution Vulnerability Detection

This notebook builds a Graph Neural Network model using Control Flow Graph (CFG) embeddings from code samples to predict speculative execution vulnerabilities.

---

## ðŸ”§ Step 1: Setup

Install required libraries and import dependencies.


# !pip install torch torch_geometric networkx pandas

import os
import re
import ast
import torch
import networkx as nx
import pandas as pd
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv, global_mean_pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


## ðŸ“Š Step 2: Parse Code and Generate CFGs

We'll use a mock parser that constructs a simplistic control flow graph (CFG) based on Python `if`, `for`, `while` constructs. 
You should replace this with a real CFG extractor (e.g., from LLVM or Joern) for real-world use.

def extract_mock_cfg(code):
    # Tokenize into lines and simulate basic CFG
    lines = code.split('\n')
    G = nx.DiGraph()
    for i, line in enumerate(lines):
        G.add_node(i, label=line.strip())
        if i < len(lines) - 1:
            G.add_edge(i, i+1)
    return G


def graph_to_pyg_data(graph, label):
    node_features = []
    for _, data in graph.nodes(data=True):
        text = data.get('label', '')
        features = [
            int('if' in text),
            int('for' in text),
            int('while' in text),
            len(text)
        ]
        node_features.append(features)

    x = torch.tensor(node_features, dtype=torch.float)
    edge_index = torch.tensor(list(graph.edges), dtype=torch.long).t().contiguous()
    y = torch.tensor([label], dtype=torch.long)

    return Data(x=x, edge_index=edge_index, y=y)


def load_dataset(path):
    dataset = []
    for label in ['vulnerable', 'safe']:
        folder = os.path.join(path, label)
        for file in os.listdir(folder):
            if file.endswith('.c') or file.endswith('.cpp') or file.endswith('.py'):
                with open(os.path.join(folder, file), 'r') as f:
                    code = f.read()
                    cfg = extract_mock_cfg(code)
                    data = graph_to_pyg_data(cfg, 1 if label == 'vulnerable' else 0)
                    dataset.append(data)
    return dataset

dataset = load_dataset('data')
train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)
train_loader = DataLoader(train_data, batch_size=8, shuffle=True)
test_loader = DataLoader(test_data, batch_size=8)


## ðŸ§  Step 3: Define GCN Model

class GCN(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.lin = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index).relu()
        x = global_mean_pool(x, batch)
        return self.lin(x)


## ðŸ‹ï¸ Step 4: Train the Model

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN(input_dim=4, hidden_dim=32, output_dim=2).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()

def train():
    model.train()
    total_loss = 0
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.batch)
        loss = criterion(out, data.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(train_loader)

for epoch in range(1, 21):
    loss = train()
    print(f'Epoch {epoch}, Loss: {loss:.4f}')


## ðŸ“ˆ Step 5: Evaluate

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for data in test_loader:
        data = data.to(device)
        out = model(data.x, data.edge_index, data.batch)
        preds = out.argmax(dim=1).cpu().numpy()
        labels = data.y.cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels)

print(classification_report(all_labels, all_preds))
