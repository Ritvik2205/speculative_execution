#!/usr/bin/env python3
"""
GitHub Vulnerability Scanner
Integrates the robust vulnerability detection system with GitHub crawled repositories
and compiled assembly code to identify vulnerable patterns in real-world code.
"""

import os
import argparse
import json
import pickle
import sqlite3
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict
import logging

from robust_vulnerability_detector import RobustVulnerabilityDetector
from semantic_vulnerability_analyzer import SemanticVulnerabilityAnalyzer
from ensemble_vulnerability_detector import EnsembleVulnerabilityDetector
from dsl_matcher import DSLMatcher
from minimal_subsequence import reduce_to_minimal_window

@dataclass
class GitHubRepository:
    """Represents a GitHub repository with metadata"""
    url: str
    owner: str
    name: str
    stars: int
    language: str
    cloned: bool = False
    compiled: bool = False
    scanned: bool = False

@dataclass
class AssemblyFile:
    """Represents a compiled assembly file"""
    filepath: str
    source_file: str
    repository: str
    architecture: str
    compiler: str
    optimization_level: str
    file_size: int
    instruction_count: int = 0

@dataclass
class VulnerabilityMatch:
    """Represents a vulnerability found in GitHub code"""
    repository: str
    source_file: str
    assembly_file: str
    vulnerability_type: str
    confidence: float
    risk_level: str
    location: Dict[str, int]
    evidence: Dict[str, Any]
    detector_used: str
    timestamp: str

class GitHubVulnerabilityScanner:
    """Main scanner that integrates GitHub crawling with vulnerability detection"""
    
    def __init__(self, work_dir: str = "."):
        self.work_dir = Path(work_dir)
        self.db_path = self.work_dir / "vulnerability_scan_results.db"
        
        # Initialize detectors
        self.robust_detector = None
        self.semantic_analyzer = SemanticVulnerabilityAnalyzer()
        self.ensemble_detector = None
        
        # Configuration
        self.config = {
            'max_repos_to_scan': 100,
            'min_confidence_threshold': 0.4,
            'supported_architectures': ['x86_64', 'arm64', 'riscv64'],  # Added riscv64
            'assembly_file_extensions': ['.s', '.asm'],
            'max_file_size_mb': 10,
            'batch_size': 10
        }
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('vulnerability_scan.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Initialize database
        self._init_database()
    
    def _init_database(self):
        """Initialize SQLite database for storing scan results"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Repositories table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS repositories (
                id INTEGER PRIMARY KEY,
                url TEXT UNIQUE,
                owner TEXT,
                name TEXT,
                stars INTEGER,
                language TEXT,
                cloned BOOLEAN DEFAULT FALSE,
                compiled BOOLEAN DEFAULT FALSE,
                scanned BOOLEAN DEFAULT FALSE,
                scan_timestamp TEXT
            )
        ''')
        
        # Assembly files table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS assembly_files (
                id INTEGER PRIMARY KEY,
                filepath TEXT UNIQUE,
                source_file TEXT,
                repository TEXT,
                architecture TEXT,
                compiler TEXT,
                optimization_level TEXT,
                file_size INTEGER,
                instruction_count INTEGER DEFAULT 0,
                processed BOOLEAN DEFAULT FALSE
            )
        ''')
        
        # Vulnerabilities table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS vulnerabilities (
                id INTEGER PRIMARY KEY,
                repository TEXT,
                source_file TEXT,
                assembly_file TEXT,
                vulnerability_type TEXT,
                confidence REAL,
                risk_level TEXT,
                location_start INTEGER,
                location_end INTEGER,
                evidence TEXT,
                detector_used TEXT,
                timestamp TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        
        self.logger.info(f"Database initialized at {self.db_path}")
    
    def load_github_repositories(self) -> List[GitHubRepository]:
        """Load GitHub repositories from the crawled data"""
        repos = []
        
        github_repos_file = self.work_dir / "github_repos.txt"
        if not github_repos_file.exists():
            self.logger.error(f"GitHub repos file not found: {github_repos_file}")
            return repos
        
        self.logger.info(f"Loading GitHub repositories from {github_repos_file}")
        
        with open(github_repos_file, 'r') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                try:
                    # Parse GitHub URL to extract owner/repo
                    if 'github.com' in line:
                        parts = line.replace('https://github.com/', '').split('/')
                        if len(parts) >= 2:
                            owner, name = parts[0], parts[1]
                            
                            repo = GitHubRepository(
                                url=line,
                                owner=owner,
                                name=name,
                                stars=0,  # Could be enhanced to fetch actual stars
                                language='C/C++',
                                cloned=self._check_if_cloned(owner, name),
                                compiled=self._check_if_compiled(owner, name)
                            )
                            repos.append(repo)
                
                except Exception as e:
                    self.logger.warning(f"Failed to parse repo at line {line_num}: {line} - {e}")
        
        self.logger.info(f"Loaded {len(repos)} repositories")
        return repos
    
    def _check_if_cloned(self, owner: str, name: str) -> bool:
        """Check if repository has been cloned"""
        repo_path = self.work_dir / "repos" / owner / name
        return repo_path.exists() and repo_path.is_dir()
    
    def _check_if_compiled(self, owner: str, name: str) -> bool:
        """Check if repository has been compiled to assembly"""
        # Check if there are assembly files for this repository
        assembly_dir = self.work_dir / "assembly_outputs"
        if not assembly_dir.exists():
            return False
        
        # Look for assembly files that might belong to this repo
        for arch_dir in assembly_dir.iterdir():
            if arch_dir.is_dir():
                for compiler_dir in arch_dir.iterdir():
                    if compiler_dir.is_dir():
                        for opt_dir in compiler_dir.iterdir():
                            if opt_dir.is_dir():
                                # Check if any files contain the repo name
                                for asm_file in opt_dir.glob("*.s"):
                                    if name.lower() in asm_file.name.lower():
                                        return True
        return False
    
    def discover_assembly_files(self) -> List[AssemblyFile]:
        """Discover all compiled assembly files from GitHub repositories"""
        assembly_files = []
        
        assembly_dir = self.work_dir / "assembly_outputs"
        if not assembly_dir.exists():
            self.logger.error(f"Assembly outputs directory not found: {assembly_dir}")
            return assembly_files
        
        self.logger.info(f"Discovering assembly files in {assembly_dir}")
        
        # Walk through assembly_outputs/arch/compiler/opt_level/
        for arch_dir in assembly_dir.iterdir():
            if not arch_dir.is_dir():
                continue
            
            architecture = arch_dir.name
            self.logger.info(f"Checking architecture: {architecture}")
            if architecture not in self.config['supported_architectures']:
                self.logger.info(f"Skipping unsupported architecture: {architecture}")
                continue
            
            for compiler_dir in arch_dir.iterdir():
                if not compiler_dir.is_dir():
                    continue
                
                compiler = compiler_dir.name
                
                for opt_dir in compiler_dir.iterdir():
                    if not opt_dir.is_dir():
                        continue
                    
                    optimization_level = opt_dir.name
                    
                    # Look for .s files recursively in this optimization directory
                    asm_files_in_dir = list(opt_dir.rglob("*.s"))
                    self.logger.info(f"Found {len(asm_files_in_dir)} .s files in {opt_dir}")
                    
                    for asm_file in asm_files_in_dir:
                        try:
                            file_size = asm_file.stat().st_size
                            
                            # Skip files that are too large
                            if file_size > self.config['max_file_size_mb'] * 1024 * 1024:
                                self.logger.info(f"Skipping large file: {asm_file.name} ({file_size} bytes)")
                                continue
                            
                            # Try to determine source file and repository
                            source_file, repository = self._infer_source_info(asm_file)
                            
                            assembly_file = AssemblyFile(
                                filepath=str(asm_file),
                                source_file=source_file,
                                repository=repository,
                                architecture=architecture,
                                compiler=compiler,
                                optimization_level=optimization_level,
                                file_size=file_size
                            )
                            
                            assembly_files.append(assembly_file)
                            
                        except Exception as e:
                            self.logger.warning(f"Failed to process {asm_file}: {e}")
        
        self.logger.info(f"Discovered {len(assembly_files)} assembly files")
        return assembly_files
    
    def _infer_source_info(self, asm_file: Path) -> Tuple[str, str]:
        """Infer source file and repository from assembly file path/name"""
        # Try to extract from filename or path
        filename = asm_file.stem
        
        # Look for patterns in the filename that might indicate source
        source_file = "unknown"
        repository = "unknown"
        
        # Check if filename contains common source file patterns
        if '.' in filename:
            potential_source = filename.split('.')[0] + '.c'
            source_file = potential_source
        
        # Try to match against known repositories
        repos_dir = self.work_dir / "repos"
        if repos_dir.exists():
            for owner_dir in repos_dir.iterdir():
                if owner_dir.is_dir():
                    for repo_dir in owner_dir.iterdir():
                        if repo_dir.is_dir():
                            repo_name = repo_dir.name
                            if repo_name.lower() in filename.lower():
                                repository = f"{owner_dir.name}/{repo_name}"
                                break
        
        return source_file, repository
    
    def initialize_detectors(self, force_retrain: bool = False):
        """Initialize and train vulnerability detectors"""
        self.logger.info("Initializing vulnerability detectors...")
        
        # Initialize robust detector
        self.robust_detector = RobustVulnerabilityDetector()
        
        # Check if we have pre-trained models
        # Prefer granular models saved by train_model.py (joblib files)
        granular_model_dir = self.work_dir / "ensemble_vulnerability_model_ensemble"
        robust_model_path = self.work_dir / "robust_vulnerability_model.pkl"
        ensemble_model_path = self.work_dir / "ensemble_vulnerability_model.pkl"
        
        if granular_model_dir.exists() and not force_retrain:
            try:
                self.logger.info("Loading ML components from ensemble_vulnerability_model_ensemble/")
                import joblib
                self.robust_detector.ml_classifier = joblib.load(granular_model_dir / 'ml_classifier.joblib')
                self.robust_detector.anomaly_detector = joblib.load(granular_model_dir / 'anomaly_detector.joblib')
                self.robust_detector.scaler = joblib.load(granular_model_dir / 'scaler.joblib')
                # Also load signatures to support pattern matching
                vuln_asm_dir = "../c_vulns/asm_code"
                signatures = self.robust_detector.analyze_vulnerable_code(vuln_asm_dir)
                self.robust_detector.vulnerability_signatures = signatures
                self.logger.info("ML models loaded. Signatures built from vulnerable corpus.")
            except Exception as e:
                self.logger.error(f"Failed to load granular models: {e}. Falling back to training...")
                force_retrain = True
        elif robust_model_path.exists() and not force_retrain:
            try:
                # For now, retrain to avoid pickle issues
                self.logger.info("Training robust detector from scratch...")
                vuln_asm_dir = "../c_vulns/asm_code"
                signatures = self.robust_detector.analyze_vulnerable_code(vuln_asm_dir)
                self.robust_detector.vulnerability_signatures = signatures
                self.robust_detector.build_ml_classifier(signatures)
                self.logger.info(f"Robust detector trained with {len(signatures)} signatures")
            except Exception as e:
                self.logger.error(f"Failed to load robust detector model: {e}")
                return False
        else:
            # Train from scratch
            self.logger.info("Training robust detector from scratch...")
            vuln_asm_dir = "../c_vulns/asm_code"
            if not os.path.exists(vuln_asm_dir):
                self.logger.error(f"Vulnerable assembly directory not found: {vuln_asm_dir}")
                return False
            
            signatures = self.robust_detector.analyze_vulnerable_code(vuln_asm_dir)
            self.robust_detector.vulnerability_signatures = signatures
            self.robust_detector.build_ml_classifier(signatures)
            self.logger.info(f"Robust detector trained with {len(signatures)} signatures")
        
        # Initialize ensemble detector
        self.ensemble_detector = EnsembleVulnerabilityDetector()
        if ensemble_model_path.exists() and not force_retrain:
            try:
                # For now, retrain to avoid pickle issues
                self.logger.info("Training ensemble detector from scratch...")
                self.ensemble_detector.train_ensemble("../c_vulns/asm_code")
                self.logger.info("Ensemble detector trained")
            except Exception as e:
                self.logger.error(f"Failed to load ensemble detector model: {e}")
                return False
        else:
            self.logger.info("Training ensemble detector from scratch...")
            self.ensemble_detector.train_ensemble("../c_vulns/asm_code")
            self.logger.info("Ensemble detector trained")
        
        return True
    
    def parse_assembly_file(self, asm_file: AssemblyFile) -> List[Dict[str, Any]]:
        """Parse assembly file into instruction format for vulnerability detection"""
        instructions = []
        
        try:
            with open(asm_file.filepath, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            for i, line in enumerate(lines):
                line = line.strip()
                
                # Skip empty lines, comments, and directives
                if not line or line.startswith('.') or line.startswith('#') or ':' in line:
                    continue
                
                # Basic instruction parsing
                parts = line.split()
                if not parts:
                    continue
                
                opcode = parts[0].lower()
                operands = parts[1:] if len(parts) > 1 else []
                
                # Analyze instruction semantics
                semantics = self._analyze_instruction_semantics(opcode, operands, asm_file.architecture)
                
                instruction = {
                    'line_num': i + 1,
                    'raw_line': line,
                    'opcode': opcode,
                    'operands': operands,
                    'semantics': semantics
                }
                
                instructions.append(instruction)
            
            # Update instruction count
            asm_file.instruction_count = len(instructions)
            
        except Exception as e:
            self.logger.error(f"Failed to parse assembly file {asm_file.filepath}: {e}")
        
        return instructions
    
    def _analyze_instruction_semantics(self, opcode: str, operands: List[str], arch: str) -> Dict[str, bool]:
        """Analyze semantic properties of an instruction"""
        semantics = {
            'is_branch': False,
            'is_conditional': False,
            'is_indirect': False,
            'is_call': False,
            'is_return': False,
            'is_load': False,
            'is_store': False,
            'accesses_memory': False,
            'is_arithmetic': False,
            'is_comparison': False,
            'is_speculation_barrier': False,
            'is_cache_operation': False,
            'is_timing_sensitive': False,
            'is_privileged': False
        }
        
        if arch == 'x86_64':
            # x86-64 instruction semantics
            if opcode.startswith('j'):
                semantics['is_branch'] = True
                if opcode != 'jmp':
                    semantics['is_conditional'] = True
                if any('[' in op for op in operands):
                    semantics['is_indirect'] = True
            elif opcode in ['call', 'ret']:
                semantics['is_call'] = opcode == 'call'
                semantics['is_return'] = opcode == 'ret'
                if opcode == 'call' and any('[' in op or '%' in op for op in operands):
                    semantics['is_indirect'] = True
            elif opcode in ['mov', 'movzx', 'movsx', 'movzbl', 'movzwl', 'lea']:
                if any('[' in op for op in operands):
                    semantics['accesses_memory'] = True
                    semantics['is_load'] = True
            elif opcode in ['add', 'sub', 'mul', 'div', 'xor', 'and', 'or', 'shl', 'shr']:
                semantics['is_arithmetic'] = True
            elif opcode in ['cmp', 'test']:
                semantics['is_comparison'] = True
            elif opcode in ['lfence', 'mfence', 'sfence']:
                semantics['is_speculation_barrier'] = True
            elif opcode in ['clflush', 'clwb', 'clflushopt']:
                semantics['is_cache_operation'] = True
            elif opcode in ['rdtsc', 'rdtscp']:
                semantics['is_timing_sensitive'] = True
                
        elif arch == 'arm64':
            # ARM64 instruction semantics
            if opcode.startswith('b'):
                semantics['is_branch'] = True
                if '.' in opcode:  # conditional branches like b.eq
                    semantics['is_conditional'] = True
                if opcode in ['br', 'blr']:
                    semantics['is_indirect'] = True
            elif opcode in ['bl', 'blr', 'ret']:
                semantics['is_call'] = opcode in ['bl', 'blr']
                semantics['is_return'] = opcode == 'ret'
                if opcode == 'blr':
                    semantics['is_indirect'] = True
            elif opcode in ['ldr', 'ldrb', 'ldrh', 'ldp']:
                semantics['is_load'] = True
                semantics['accesses_memory'] = True
            elif opcode in ['str', 'strb', 'strh', 'stp']:
                semantics['is_store'] = True
                semantics['accesses_memory'] = True
            elif opcode in ['add', 'sub', 'mul', 'div', 'and', 'orr', 'eor', 'lsl', 'lsr']:
                semantics['is_arithmetic'] = True
            elif opcode in ['cmp', 'subs']:
                semantics['is_comparison'] = True
            elif opcode in ['dsb', 'isb', 'dmb']:
                semantics['is_speculation_barrier'] = True
            elif opcode in ['dc', 'ic']:
                semantics['is_cache_operation'] = True
            elif opcode == 'mrs':
                semantics['is_timing_sensitive'] = True
                semantics['is_privileged'] = True
        
        return semantics
    
    def scan_assembly_file(self, asm_file: AssemblyFile, detector_type: str = "ensemble") -> List[VulnerabilityMatch]:
        """Scan a single assembly file for vulnerabilities"""
        matches = []
        
        try:
            # Parse assembly file
            instructions = self.parse_assembly_file(asm_file)
            if not instructions:
                return matches
            
            self.logger.info(f"Scanning {asm_file.filepath} ({len(instructions)} instructions)")
            
            # Run vulnerability detection
            detections = []
            
            if detector_type == "robust" and self.robust_detector:
                detections = self.robust_detector.detect_vulnerabilities(instructions, asm_file.architecture)
                detector_used = "robust"
            elif detector_type == "semantic":
                detections = self.semantic_analyzer.detect_semantic_vulnerabilities(instructions)
                detector_used = "semantic"
            elif detector_type == "ensemble" and self.ensemble_detector:
                detections = self.ensemble_detector.detect_vulnerabilities(instructions, asm_file.architecture)
                detector_used = "ensemble"
            else:
                self.logger.warning(f"Invalid detector type: {detector_type}")
                return matches
            
            # Convert detections to vulnerability matches
            for detection in detections:
                # Handle different detection formats
                if hasattr(detection, 'vuln_type'):  # Ensemble detection
                    vuln_type = detection.vuln_type
                    confidence = detection.confidence
                    risk_level = detection.risk_assessment
                    location = detection.location
                    evidence = detection.evidence
                elif isinstance(detection, dict):  # Robust/semantic detection
                    vuln_type = detection.get('vuln_type', detection.get('vulnerability_types', ['UNKNOWN'])[0])
                    if isinstance(vuln_type, list):
                        vuln_type = vuln_type[0] if vuln_type else 'UNKNOWN'
                    confidence = detection.get('confidence', detection.get('primary_confidence', 0.0))
                    risk_level = self._assess_risk_level(confidence)
                    location = detection.get('location', {'start_line': 0, 'end_line': 0})
                    evidence = detection.get('evidence', {})
                else:
                    continue
                
                # Filter by confidence threshold
                if confidence < self.config['min_confidence_threshold']:
                    continue

                # Final DSL validation and minimal shrink (best-effort)
                try:
                    # Reconstruct window from location
                    start_line = location.get('start_line', 0)
                    end_line = location.get('end_line', 0)
                    if start_line and end_line:
                        window = [instr for instr in instructions if start_line <= instr.get('line_num', 0) <= end_line]
                        minimized, dsl_evidence = reduce_to_minimal_window(window, vuln_type, asm_file.architecture)
                        if minimized:
                            new_start = minimized[0].get('line_num', start_line)
                            new_end = minimized[-1].get('line_num', end_line)
                            location = {'start_line': new_start, 'end_line': new_end}
                            # attach evidence
                            evidence = evidence or {}
                            evidence['dsl'] = dsl_evidence
                            evidence['minimized_length'] = len(minimized)
                except Exception:
                    pass
                
                match = VulnerabilityMatch(
                    repository=asm_file.repository,
                    source_file=asm_file.source_file,
                    assembly_file=asm_file.filepath,
                    vulnerability_type=vuln_type,
                    confidence=confidence,
                    risk_level=risk_level,
                    location=location,
                    evidence=evidence,
                    detector_used=detector_used,
                    timestamp=self._get_timestamp()
                )
                
                matches.append(match)
        
        except Exception as e:
            self.logger.error(f"Failed to scan {asm_file.filepath}: {e}")
        
        return matches
    
    def _assess_risk_level(self, confidence: float) -> str:
        """Assess risk level based on confidence score"""
        if confidence >= 0.8:
            return "CRITICAL"
        elif confidence >= 0.6:
            return "HIGH"
        elif confidence >= 0.4:
            return "MEDIUM"
        else:
            return "LOW"
    
    def _get_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def save_results_to_database(self, matches: List[VulnerabilityMatch]):
        """Save vulnerability matches to database"""
        if not matches:
            return
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for match in matches:
            cursor.execute('''
                INSERT OR REPLACE INTO vulnerabilities 
                (repository, source_file, assembly_file, vulnerability_type, confidence, 
                 risk_level, location_start, location_end, evidence, detector_used, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                match.repository,
                match.source_file,
                match.assembly_file,
                match.vulnerability_type,
                match.confidence,
                match.risk_level,
                match.location.get('start_line', 0),
                match.location.get('end_line', 0),
                json.dumps(match.evidence),
                match.detector_used,
                match.timestamp
            ))
        
        conn.commit()
        conn.close()
        
        self.logger.info(f"Saved {len(matches)} vulnerability matches to database")
    
    def run_full_scan(self, detector_type: str = "ensemble", max_files: int = None) -> Dict[str, Any]:
        """Run full vulnerability scan on all GitHub repositories"""
        self.logger.info("Starting full GitHub vulnerability scan...")
        
        # Initialize detectors
        if not self.initialize_detectors():
            self.logger.error("Failed to initialize detectors")
            return {}
        
        # Discover assembly files
        assembly_files = self.discover_assembly_files()
        if not assembly_files:
            self.logger.error("No assembly files found")
            return {}
        
        # Limit files if specified
        if max_files:
            assembly_files = assembly_files[:max_files]
        
        # Scan files
        all_matches = []
        scan_stats = {
            'total_files': len(assembly_files),
            'scanned_files': 0,
            'failed_files': 0,
            'total_vulnerabilities': 0,
            'vulnerabilities_by_type': defaultdict(int),
            'vulnerabilities_by_risk': defaultdict(int),
            'vulnerabilities_by_repo': defaultdict(int)
        }
        
        for i, asm_file in enumerate(assembly_files):
            self.logger.info(f"Scanning file {i+1}/{len(assembly_files)}: {Path(asm_file.filepath).name}")
            
            try:
                matches = self.scan_assembly_file(asm_file, detector_type)
                
                if matches:
                    all_matches.extend(matches)
                    self.save_results_to_database(matches)
                    
                    # Update statistics
                    for match in matches:
                        scan_stats['total_vulnerabilities'] += 1
                        scan_stats['vulnerabilities_by_type'][match.vulnerability_type] += 1
                        scan_stats['vulnerabilities_by_risk'][match.risk_level] += 1
                        scan_stats['vulnerabilities_by_repo'][match.repository] += 1
                
                scan_stats['scanned_files'] += 1
                
            except Exception as e:
                self.logger.error(f"Failed to scan {asm_file.filepath}: {e}")
                scan_stats['failed_files'] += 1
        
        # Generate summary report
        self.generate_scan_report(scan_stats, all_matches)
        
        self.logger.info(f"Scan complete! Found {scan_stats['total_vulnerabilities']} vulnerabilities in {scan_stats['scanned_files']} files")
        
        return scan_stats
    
    def generate_scan_report(self, stats: Dict[str, Any], matches: List[VulnerabilityMatch]):
        """Generate comprehensive scan report"""
        report = {
            'scan_summary': stats,
            'top_vulnerable_repositories': dict(list(stats['vulnerabilities_by_repo'].items())[:10]),
            'vulnerability_breakdown': dict(stats['vulnerabilities_by_type']),
            'risk_distribution': dict(stats['vulnerabilities_by_risk']),
            'high_risk_vulnerabilities': [
                {
                    'repository': m.repository,
                    'source_file': m.source_file,
                    'vulnerability_type': m.vulnerability_type,
                    'confidence': m.confidence,
                    'risk_level': m.risk_level
                }
                for m in matches if m.risk_level in ['CRITICAL', 'HIGH']
            ][:20]  # Top 20 high-risk vulnerabilities
        }
        
        # Save report
        report_file = self.work_dir / "github_vulnerability_scan_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"Scan report saved to {report_file}")
        
        # Print summary
        print("\n" + "="*80)
        print("GITHUB VULNERABILITY SCAN RESULTS")
        print("="*80)
        print(f"üìä Scanned {stats['scanned_files']} assembly files")
        print(f"üîç Found {stats['total_vulnerabilities']} potential vulnerabilities")
        print(f"‚ùå Failed to scan {stats['failed_files']} files")
        
        print(f"\nüéØ Vulnerability Types:")
        for vuln_type, count in stats['vulnerabilities_by_type'].items():
            print(f"   {vuln_type}: {count}")
        
        print(f"\n‚ö†Ô∏è  Risk Distribution:")
        for risk_level, count in stats['vulnerabilities_by_risk'].items():
            print(f"   {risk_level}: {count}")
        
        print(f"\nüèÜ Top Vulnerable Repositories:")
        for repo, count in list(stats['vulnerabilities_by_repo'].items())[:5]:
            print(f"   {repo}: {count} vulnerabilities")
        
        return report

def main():
    """Main execution function"""
    print("üöÄ GitHub Vulnerability Scanner")
    print("Integrating robust vulnerability detection with GitHub crawled repositories")

    parser = argparse.ArgumentParser(description="Scan GitHub-compiled assembly for speculative execution vulnerabilities")
    parser.add_argument("--num-files", type=int, default=None, help="Number of assembly files to process. If omitted or <=0, process all.")
    parser.add_argument("--detector", type=str, default="ensemble", choices=["ensemble", "robust", "semantic"], help="Detector type to use")
    args = parser.parse_args()

    # Normalize num-files: None or <=0 means process all
    max_files = args.num_files if (args.num_files and args.num_files > 0) else None

    scanner = GitHubVulnerabilityScanner()

    # Run scan with optional file limit
    results = scanner.run_full_scan(
        detector_type=args.detector,
        max_files=max_files
    )
    
    print(f"\n‚úÖ Scan completed successfully!")
    print(f"   Database: {scanner.db_path}")
    print(f"   Report: github_vulnerability_scan_report.json")

if __name__ == "__main__":
    main()