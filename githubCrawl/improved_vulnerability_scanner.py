#!/usr/bin/env python3
"""
Improved Vulnerability Scanner
Enhanced accuracy with reduced false positives through better feature engineering and validation
"""

import os
import re
import json
import sqlite3
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import logging
import pickle
import numpy as np
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import networkx as nx

# Make capstone optional
try:
    import capstone
    CAPSTONE_AVAILABLE = True
except ImportError:
    CAPSTONE_AVAILABLE = False
    print("Warning: capstone not available, using simplified parsing")

@dataclass
class ImprovedVulnerabilityDetection:
    """Enhanced vulnerability detection with confidence metrics"""
    assembly_file: str
    vulnerability_type: str
    confidence: float
    validation_score: float  # NEW: Pre-validation confidence
    evidence: Dict[str, Any]
    location_start: int
    location_end: int
    risk_level: str
    false_positive_likelihood: float  # NEW: Estimated FP probability
    exploit_requirements: List[str]  # NEW: What's needed for exploitation
    mitigation_factors: List[str]  # NEW: What reduces exploitability

@dataclass
class CodeContext:
    """Contextual information about the code being analyzed"""
    function_name: str
    is_public_api: bool
    handles_user_input: bool
    is_security_critical: bool
    has_bounds_checks: bool
    has_speculation_barriers: bool
    memory_safety_level: str  # SAFE, MODERATE, UNSAFE
    compiler_optimizations: str  # O0, O1, O2, O3
    architecture: str

class ImprovedVulnerabilityScanner:
    """Enhanced vulnerability scanner with better accuracy"""
    
    def __init__(self, vuln_asm_dir: str = "../c_vulns/asm_code"):
        self.vuln_asm_dir = vuln_asm_dir
        self.logger = self._setup_logging()
        
        # Enhanced feature extractors
        self.tfidf_vectorizer = TfidfVectorizer(
            ngram_range=(1, 3),  # Include 3-grams for better context
            max_features=2000,
            stop_words=None,
            lowercase=True
        )
        
        # Multiple specialized classifiers
        self.classifiers = {
            'spectre_v1': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),
            'spectre_v2': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),
            'meltdown': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),
            'l1tf': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),
            'bhi': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),
            'mds': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),
            'retbleed': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),
            'inception': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)
        }
        
        # Anomaly detection for unknown patterns
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        
        # Feature scaler
        self.scaler = StandardScaler()
        
        # Training data storage
        self.training_signatures = []
        self.negative_samples = []  # NEW: Explicitly collect non-vulnerable code
        
        # Enhanced pattern matching
        self.vulnerability_patterns = self._load_enhanced_patterns()
        
        # Context analyzer
        self.context_analyzer = CodeContextAnalyzer()
        
        # Validation thresholds (stricter for production)
        self.confidence_thresholds = {
            'CRITICAL': 0.85,  # Very high confidence required
            'HIGH': 0.75,      # High confidence required
            'MEDIUM': 0.65,    # Medium confidence required
            'LOW': 0.50        # Low confidence threshold
        }
        
        self.logger.info("Improved Vulnerability Scanner initialized")
    
    def _parse_vulnerability_type_from_filename(self, filename: str) -> Optional[str]:
        """Parse vulnerability type from assembly filename"""
        filename_lower = filename.lower()
        
        # Map filename patterns to vulnerability types
        vuln_patterns = {
            'spectre_1': 'SPECTRE_V1',
            'spectre_v1': 'SPECTRE_V1', 
            'spectre_2': 'SPECTRE_V2',
            'meltdown': 'MELTDOWN',
            'l1tf': 'L1TF',
            'bhi': 'BHI',
            'mds': 'MDS',
            'retbleed': 'RETBLEED',
            'inception': 'INCEPTION'
        }
        
        for pattern, vuln_type in vuln_patterns.items():
            if pattern in filename_lower:
                return vuln_type
        
        return None
    
    def _parse_assembly_file(self, filepath: str) -> List[Dict[str, Any]]:
        """Parse assembly file and extract instructions"""
        instructions = []
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            lines = content.split('\n')
            address = 0
            
            for line in lines:
                line = line.strip()
                
                # Skip empty lines and comments
                if not line or line.startswith('#') or line.startswith(';') or line.startswith('//'):
                    continue
                
                # Skip assembler directives
                if line.startswith('.'):
                    continue
                
                # Skip labels (lines ending with ':')
                if line.endswith(':'):
                    continue
                
                # Parse instruction
                parts = line.split(None, 1)
                if len(parts) >= 1:
                    mnemonic = parts[0].lower()
                    op_str = parts[1] if len(parts) > 1 else ''
                    
                    instruction = {
                        'mnemonic': mnemonic,
                        'op_str': op_str,
                        'address': address,
                        'bytes': b'',  # Would need actual parsing for real bytes
                        'size': 4  # Assume 4-byte instructions
                    }
                    
                    instructions.append(instruction)
                    address += 4
            
            return instructions
            
        except Exception as e:
            self.logger.warning(f"Failed to parse assembly file {filepath}: {e}")
            return []
    
    def _extract_statistical_features(self, instructions: List[Dict]) -> Dict[str, float]:
        """Extract statistical features from instructions"""
        if not instructions:
            return {}
        
        features = {}
        total_instructions = len(instructions)
        
        # Count instruction types
        instruction_counts = defaultdict(int)
        for instr in instructions:
            mnemonic = instr.get('mnemonic', '').lower()
            instruction_counts[mnemonic] += 1
        
        # Basic statistics
        features['total_instructions'] = total_instructions
        features['unique_instructions'] = len(instruction_counts)
        features['instruction_diversity'] = len(instruction_counts) / total_instructions if total_instructions > 0 else 0
        
        # Instruction type ratios
        branch_instructions = ['b', 'bl', 'br', 'blr', 'ret', 'cbz', 'cbnz', 'tbz', 'tbnz'] + \
                            [f'b.{cond}' for cond in ['eq', 'ne', 'lt', 'le', 'gt', 'ge', 'hi', 'hs', 'lo', 'ls']]
        memory_instructions = ['ldr', 'str', 'ldp', 'stp', 'ldrb', 'strb', 'ldrh', 'strh']
        arithmetic_instructions = ['add', 'sub', 'mul', 'div', 'and', 'orr', 'eor', 'lsl', 'lsr', 'asr']
        
        branch_count = sum(instruction_counts[instr] for instr in branch_instructions if instr in instruction_counts)
        memory_count = sum(instruction_counts[instr] for instr in memory_instructions if instr in instruction_counts)
        arithmetic_count = sum(instruction_counts[instr] for instr in arithmetic_instructions if instr in instruction_counts)
        
        features['branch_ratio'] = branch_count / total_instructions
        features['memory_ratio'] = memory_count / total_instructions  
        features['arithmetic_ratio'] = arithmetic_count / total_instructions
        
        # Control flow complexity
        conditional_branches = sum(instruction_counts[instr] for instr in instruction_counts 
                                 if instr.startswith('b.') or instr in ['cbz', 'cbnz', 'tbz', 'tbnz'])
        features['conditional_branch_ratio'] = conditional_branches / total_instructions
        
        # Memory access patterns
        load_instructions = sum(instruction_counts[instr] for instr in instruction_counts if 'ldr' in instr)
        store_instructions = sum(instruction_counts[instr] for instr in instruction_counts if 'str' in instr)
        features['load_store_ratio'] = load_instructions / max(store_instructions, 1)
        
        return features
    
    def _extract_pattern_features(self, instructions: List[Dict], vuln_type: str) -> Dict[str, float]:
        """Extract vulnerability-specific pattern features"""
        features = {}
        
        if not instructions:
            return features
        
        # Convert instructions to text for pattern matching
        instr_text = '\n'.join([f"{instr.get('mnemonic', '')} {instr.get('op_str', '')}" 
                               for instr in instructions])
        
        # Vulnerability-specific patterns
        if vuln_type == 'SPECTRE_V1':
            # Look for bounds check bypass patterns
            features['bounds_check_pattern'] = len(re.findall(r'cmp.*\n.*b\.[lg].*\n.*ldr.*\[', instr_text, re.IGNORECASE))
            features['speculative_load_pattern'] = len(re.findall(r'ldr.*\[.*\].*\n.*ldr.*\[', instr_text, re.IGNORECASE))
            features['array_access_count'] = instr_text.count('[') + instr_text.count(']')
            
        elif vuln_type == 'SPECTRE_V2':
            features['indirect_branch_count'] = len(re.findall(r'\b(?:br|blr|jmp.*%)', instr_text, re.IGNORECASE))
            features['return_count'] = instr_text.lower().count('ret')
            features['call_ret_pattern'] = len(re.findall(r'(?:bl|call).*\n.*ret', instr_text, re.IGNORECASE))
            
        elif vuln_type == 'L1TF':
            features['fault_instruction_count'] = len(re.findall(r'(?:brk|hvc|svc)', instr_text, re.IGNORECASE))
            features['memory_mapping_pattern'] = len(re.findall(r'(?:mmap|munmap|mprotect)', instr_text, re.IGNORECASE))
            features['cache_flush_count'] = len(re.findall(r'(?:dc.*civac|clflush)', instr_text, re.IGNORECASE))
            
        elif vuln_type == 'BHI':
            features['branch_sequence_count'] = len(re.findall(r'(?:b\.[a-z]+.*\n){3,}', instr_text, re.IGNORECASE))
            features['complex_branch_pattern'] = len(re.findall(r'(?:b\.[a-z]+.*\n)+.*(?:br|blr)', instr_text, re.IGNORECASE))
            
        elif vuln_type == 'MDS':
            features['store_load_dependency'] = len(re.findall(r'(?:str|stp).*\n.*(?:ldr|ldp)', instr_text, re.IGNORECASE))
            features['memory_disambiguation'] = len(re.findall(r'(?:ldr|str).*\[.*\].*\n.*(?:ldr|str).*\[', instr_text, re.IGNORECASE))
        
        return features
    
    def _extract_semantic_features(self, instructions: List[Dict]) -> List[str]:
        """Extract semantic features as text tokens"""
        semantic_tokens = []
        
        for instr in instructions:
            mnemonic = instr.get('mnemonic', '').lower()
            op_str = instr.get('op_str', '').lower()
            
            # Add mnemonic
            semantic_tokens.append(mnemonic)
            
            # Add semantic categories
            if mnemonic in ['ldr', 'str', 'ldp', 'stp']:
                semantic_tokens.append('memory_access')
                if '[' in op_str:
                    semantic_tokens.append('indirect_memory')
            
            if mnemonic.startswith('b'):
                semantic_tokens.append('branch_instruction')
                if mnemonic.startswith('b.'):
                    semantic_tokens.append('conditional_branch')
            
            if mnemonic in ['add', 'sub', 'mul', 'div']:
                semantic_tokens.append('arithmetic_operation')
            
            # Extract operand patterns
            if 'x' in op_str and any(char.isdigit() for char in op_str):
                semantic_tokens.append('register_operation')
            
            if '#' in op_str:
                semantic_tokens.append('immediate_value')
        
        return semantic_tokens
    
    def _validate_vulnerability_patterns(self, instructions: List[Dict], vuln_type: str) -> Dict[str, Any]:
        """Validate vulnerability patterns against instructions"""
        evidence = {'matches': [], 'anti_matches': []}
        
        # Convert to text for regex matching
        instr_text = '\n'.join([f"{instr.get('mnemonic', '')} {instr.get('op_str', '')}" 
                               for instr in instructions])
        
        # Get patterns from config
        vuln_patterns = self.vulnerability_patterns.get(vuln_type, {})
        
        # Check required patterns
        for pattern_name, pattern_regex in vuln_patterns.get('indicators', []):
            matches = len(re.findall(pattern_regex, instr_text, re.IGNORECASE | re.MULTILINE))
            if matches > 0:
                evidence['matches'].append({
                    'pattern': pattern_name,
                    'matches': matches
                })
        
        # Check anti-patterns (things that indicate it's NOT vulnerable)
        for pattern_name, pattern_regex in vuln_patterns.get('anti_patterns', []):
            matches = len(re.findall(pattern_regex, instr_text, re.IGNORECASE | re.MULTILINE))
            if matches > 0:
                evidence['anti_matches'].append({
                    'pattern': pattern_name,
                    'matches': matches
                })
        
        return evidence
    
    def _setup_logging(self) -> logging.Logger:
        """Setup enhanced logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('improved_vulnerability_scan.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def train_improved_model(self) -> bool:
        """Train enhanced models with negative samples and validation"""
        self.logger.info("Training improved vulnerability detection models...")
        
        try:
            # Load and process vulnerable code
            vulnerable_signatures = self._extract_vulnerable_signatures()
            if not vulnerable_signatures:
                self.logger.error("No vulnerable signatures found for training")
                return False
            
            # Generate negative samples from safe code patterns
            negative_signatures = self._generate_negative_samples()
            self.logger.info(f"Generated {len(negative_signatures)} negative samples")
            
            # Combine positive and negative samples
            all_signatures = vulnerable_signatures + negative_signatures
            labels = ([1] * len(vulnerable_signatures)) + ([0] * len(negative_signatures))
            
            # Extract enhanced features
            features = []
            semantic_features = []
            
            for sig in all_signatures:
                # Extract multiple feature types
                statistical_feat = self._extract_statistical_features(sig.instructions)
                pattern_feat = self._extract_pattern_features(sig.instructions, sig.vulnerability_type)
                semantic_feat = self._extract_semantic_features(sig.instructions)
                context_feat = self._extract_context_features(sig.instructions)
                
                # Combine all features
                combined_feat = {**statistical_feat, **pattern_feat, **context_feat}
                features.append(list(combined_feat.values()))
                semantic_features.append(semantic_feat)
            
            # Convert to numpy arrays
            X_features = np.array(features)
            X_semantic = np.array(semantic_features)
            y = np.array(labels)
            
            # Scale features
            X_features_scaled = self.scaler.fit_transform(X_features)
            
            # Train TF-IDF on semantic features
            semantic_texts = [' '.join(feat) for feat in semantic_features]
            X_tfidf = self.tfidf_vectorizer.fit_transform(semantic_texts)
            
            # Train vulnerability-specific classifiers
            for vuln_type in self.classifiers.keys():
                # Create binary labels for this vulnerability type
                vuln_labels = []
                vuln_indices = []
                
                for i, sig in enumerate(all_signatures):
                    if i < len(vulnerable_signatures):  # Positive samples
                        if vulnerable_signatures[i].vulnerability_type.lower() == vuln_type.lower():
                            vuln_labels.append(1)
                        else:
                            vuln_labels.append(0)
                    else:  # Negative samples
                        vuln_labels.append(0)
                    vuln_indices.append(i)
                
                # Train if we have positive samples for this vulnerability
                if sum(vuln_labels) > 0:
                    # Combine scaled features with TF-IDF
                    X_combined = np.hstack([X_features_scaled, X_tfidf.toarray()])
                    
                    self.classifiers[vuln_type].fit(X_combined, vuln_labels)
                    self.logger.info(f"Trained {vuln_type} classifier with {sum(vuln_labels)} positive samples")
            
            # Train anomaly detector on vulnerable code only
            vuln_features = X_features_scaled[:len(vulnerable_signatures)]
            self.anomaly_detector.fit(vuln_features)
            
            # Save trained models
            self._save_improved_models()
            
            # Validate training performance
            self._validate_training_performance(X_features_scaled, X_tfidf, y, all_signatures)
            
            self.logger.info("Improved model training completed successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Training failed: {e}")
            return False
    
    def _extract_vulnerable_signatures(self) -> List['VulnerabilitySignature']:
        """Extract signatures from known vulnerable assembly code"""
        signatures = []
        
        if not os.path.exists(self.vuln_asm_dir):
            self.logger.warning(f"Vulnerable assembly directory not found: {self.vuln_asm_dir}")
            return signatures
        
        for asm_file in Path(self.vuln_asm_dir).glob("*.s"):
            try:
                # Parse vulnerability type from filename
                vuln_type = self._parse_vulnerability_type_from_filename(asm_file.name)
                if not vuln_type:
                    continue
                
                # Extract instructions
                instructions = self._parse_assembly_file(str(asm_file))
                if not instructions:
                    continue
                
                # Extract comprehensive features
                statistical_features = self._extract_statistical_features(instructions)
                pattern_features = self._extract_pattern_features(instructions, vuln_type)
                semantic_features = self._extract_semantic_features(instructions)
                
                # Create signature
                signature = VulnerabilitySignature(
                    vulnerability_type=vuln_type,
                    instructions=instructions,
                    statistical_features=statistical_features,
                    pattern_features=pattern_features,
                    semantic_features=semantic_features,
                    source_file=str(asm_file)
                )
                
                signatures.append(signature)
                self.logger.debug(f"Extracted signature for {vuln_type} from {asm_file.name}")
                
            except Exception as e:
                self.logger.warning(f"Failed to process {asm_file}: {e}")
        
        self.logger.info(f"Extracted {len(signatures)} vulnerable signatures")
        return signatures
    
    def _generate_negative_samples(self) -> List['VulnerabilitySignature']:
        """Generate negative samples from safe code patterns"""
        negative_samples = []
        
        # Safe code patterns that should NOT be detected as vulnerable
        safe_patterns = [
            # Simple arithmetic loops
            ["mov x0, #0", "mov x1, #10", "add x0, x0, #1", "cmp x0, x1", "b.lt loop"],
            
            # Safe array access with bounds checking
            ["cmp w0, w1", "b.hs bounds_error", "ldr w2, [x2, w0, uxtw #2]", "ret"],
            
            # Function call sequences
            ["bl malloc", "cbz x0, error", "bl free", "ret"],
            
            # Register saving/restoring
            ["stp x29, x30, [sp, #-16]!", "mov x29, sp", "ldp x29, x30, [sp], #16", "ret"],
            
            # Safe memory operations
            ["mov x0, #0", "str x0, [x1]", "dsb sy", "ret"],
            
            # Mathematical computations
            ["fmul d0, d0, d1", "fadd d0, d0, d2", "ret"],
            
            # String operations
            ["ldrb w0, [x1], #1", "cmp w0, #0", "b.ne loop", "ret"]
        ]
        
        for i, pattern in enumerate(safe_patterns):
            # Convert to instruction format
            instructions = []
            for instr in pattern:
                instructions.append({
                    'mnemonic': instr.split()[0],
                    'op_str': ' '.join(instr.split()[1:]) if len(instr.split()) > 1 else '',
                    'bytes': b'',
                    'address': i * 4
                })
            
            # Create negative signature
            statistical_features = self._extract_statistical_features(instructions)
            pattern_features = self._extract_pattern_features(instructions, 'SAFE')
            semantic_features = self._extract_semantic_features(instructions)
            
            signature = VulnerabilitySignature(
                vulnerability_type='SAFE',
                instructions=instructions,
                statistical_features=statistical_features,
                pattern_features=pattern_features,
                semantic_features=semantic_features,
                source_file=f'safe_pattern_{i}'
            )
            
            negative_samples.append(signature)
        
        return negative_samples
    
    def _extract_context_features(self, instructions: List[Dict]) -> Dict[str, float]:
        """Extract contextual features about the code"""
        features = {}
        
        if not instructions:
            return features
        
        # Function structure analysis
        features['has_function_prologue'] = self._has_function_prologue(instructions)
        features['has_function_epilogue'] = self._has_function_epilogue(instructions)
        features['function_complexity'] = self._calculate_function_complexity(instructions)
        
        # Security-relevant features
        features['has_bounds_checks'] = self._has_bounds_checks(instructions)
        features['has_speculation_barriers'] = self._has_speculation_barriers(instructions)
        features['has_stack_protection'] = self._has_stack_protection(instructions)
        
        # Code quality indicators
        features['register_usage_diversity'] = self._calculate_register_diversity(instructions)
        features['instruction_entropy'] = self._calculate_instruction_entropy(instructions)
        features['control_flow_complexity'] = self._calculate_control_flow_complexity(instructions)
        
        return features
    
    def _has_function_prologue(self, instructions: List[Dict]) -> float:
        """Check for standard function prologue patterns"""
        if len(instructions) < 3:
            return 0.0
        
        # Look for stack frame setup patterns
        prologue_patterns = [
            r'stp.*x29.*x30.*sp',  # ARM64 prologue
            r'push.*rbp',          # x86_64 prologue
            r'mov.*x29.*sp'        # Frame pointer setup
        ]
        
        first_instructions = ' '.join([instr.get('mnemonic', '') + ' ' + instr.get('op_str', '') 
                                     for instr in instructions[:3]])
        
        for pattern in prologue_patterns:
            if re.search(pattern, first_instructions, re.IGNORECASE):
                return 1.0
        
        return 0.0
    
    def _has_function_epilogue(self, instructions: List[Dict]) -> float:
        """Check for standard function epilogue patterns"""
        if len(instructions) < 3:
            return 0.0
        
        # Look for stack frame teardown patterns
        epilogue_patterns = [
            r'ldp.*x29.*x30.*sp',  # ARM64 epilogue
            r'pop.*rbp',           # x86_64 epilogue
            r'ret'                 # Return instruction
        ]
        
        last_instructions = ' '.join([instr.get('mnemonic', '') + ' ' + instr.get('op_str', '') 
                                    for instr in instructions[-3:]])
        
        for pattern in epilogue_patterns:
            if re.search(pattern, last_instructions, re.IGNORECASE):
                return 1.0
        
        return 0.0
    
    def _has_bounds_checks(self, instructions: List[Dict]) -> float:
        """Detect explicit bounds checking patterns"""
        bounds_check_score = 0.0
        
        for i in range(len(instructions) - 1):
            current = instructions[i]
            next_instr = instructions[i + 1]
            
            # Look for compare followed by conditional branch
            if (current.get('mnemonic', '').lower() == 'cmp' and 
                next_instr.get('mnemonic', '').lower().startswith('b.')):
                bounds_check_score += 0.3
        
        return min(bounds_check_score, 1.0)
    
    def _has_speculation_barriers(self, instructions: List[Dict]) -> float:
        """Detect speculation barrier instructions"""
        barrier_instructions = ['dsb', 'isb', 'lfence', 'mfence', 'sfence']
        
        barrier_count = 0
        for instr in instructions:
            mnemonic = instr.get('mnemonic', '').lower()
            if mnemonic in barrier_instructions:
                barrier_count += 1
        
        return min(barrier_count / len(instructions) * 10, 1.0) if instructions else 0.0
    
    def _has_stack_protection(self, instructions: List[Dict]) -> float:
        """Detect stack protection mechanisms"""
        protection_patterns = [
            r'__stack_chk_guard',
            r'__stack_chk_fail',
            r'canary',
            r'stack.*guard'
        ]
        
        all_text = ' '.join([instr.get('op_str', '') for instr in instructions])
        
        for pattern in protection_patterns:
            if re.search(pattern, all_text, re.IGNORECASE):
                return 1.0
        
        return 0.0
    
    def _calculate_register_diversity(self, instructions: List[Dict]) -> float:
        """Calculate how many different registers are used"""
        registers = set()
        
        for instr in instructions:
            op_str = instr.get('op_str', '')
            # Extract register names (simplified)
            reg_matches = re.findall(r'\b[xw]\d+\b|\b[re][a-z][a-z]\b', op_str)
            registers.update(reg_matches)
        
        # Normalize by typical register usage
        return min(len(registers) / 10.0, 1.0)
    
    def _calculate_instruction_entropy(self, instructions: List[Dict]) -> float:
        """Calculate entropy of instruction distribution"""
        if not instructions:
            return 0.0
        
        # Count instruction types
        instr_counts = Counter()
        for instr in instructions:
            instr_counts[instr.get('mnemonic', '')] += 1
        
        # Calculate entropy
        total = len(instructions)
        entropy = 0.0
        for count in instr_counts.values():
            p = count / total
            if p > 0:
                entropy -= p * np.log2(p)
        
        # Normalize to 0-1 range
        max_entropy = np.log2(len(instr_counts)) if len(instr_counts) > 1 else 1
        return entropy / max_entropy if max_entropy > 0 else 0.0
    
    def _calculate_control_flow_complexity(self, instructions: List[Dict]) -> float:
        """Calculate control flow complexity"""
        branch_instructions = 0
        total_instructions = len(instructions)
        
        if total_instructions == 0:
            return 0.0
        
        for instr in instructions:
            mnemonic = instr.get('mnemonic', '').lower()
            if (mnemonic.startswith('b') or mnemonic.startswith('j') or 
                mnemonic in ['call', 'ret', 'br', 'blr']):
                branch_instructions += 1
        
        return branch_instructions / total_instructions
    
    def _calculate_function_complexity(self, instructions: List[Dict]) -> float:
        """Calculate overall function complexity score"""
        if not instructions:
            return 0.0
        
        # Cyclomatic complexity approximation
        decision_points = 0
        for instr in instructions:
            mnemonic = instr.get('mnemonic', '').lower()
            # Count conditional branches as decision points
            if mnemonic.startswith('b.') and mnemonic != 'b':
                decision_points += 1
        
        # Normalize by function size
        complexity = (decision_points + 1) / len(instructions) * 100
        return min(complexity, 1.0)
    
    def detect_vulnerabilities_improved(self, assembly_file: str, 
                                      context: Optional[CodeContext] = None) -> List[ImprovedVulnerabilityDetection]:
        """Enhanced vulnerability detection with validation"""
        detections = []
        
        try:
            # Parse assembly file
            instructions = self._parse_assembly_file(assembly_file)
            if not instructions:
                self.logger.warning(f"No instructions found in {assembly_file}")
                return detections
            
            # Extract code context if not provided
            if context is None:
                context = self.context_analyzer.analyze_context(assembly_file, instructions)
            
            # Pre-filter obviously safe code
            if self._is_obviously_safe(instructions, context):
                self.logger.debug(f"Skipping obviously safe code in {assembly_file}")
                return detections
            
            # Extract enhanced features
            statistical_features = self._extract_statistical_features(instructions)
            semantic_features = self._extract_semantic_features(instructions)
            context_features = self._extract_context_features(instructions)
            
            # Combine features for prediction
            combined_features = {**statistical_features, **context_features}
            feature_vector = np.array([list(combined_features.values())])
            feature_vector_scaled = self.scaler.transform(feature_vector)
            
            # Get semantic features for TF-IDF
            semantic_text = ' '.join(semantic_features)
            tfidf_vector = self.tfidf_vectorizer.transform([semantic_text])
            
            # Combine all features
            X_combined = np.hstack([feature_vector_scaled, tfidf_vector.toarray()])
            
            # Test each vulnerability type
            for vuln_type, classifier in self.classifiers.items():
                try:
                    # Get prediction probability
                    proba = classifier.predict_proba(X_combined)[0]
                    if len(proba) > 1:  # Binary classifier with positive class
                        confidence = proba[1]
                    else:
                        continue
                    
                    # Extract pattern-specific evidence
                    pattern_features = self._extract_pattern_features(instructions, vuln_type.upper())
                    pattern_evidence = self._validate_vulnerability_patterns(instructions, vuln_type.upper())
                    
                    # Calculate validation score
                    validation_score = self._calculate_validation_score(
                        confidence, pattern_evidence, context, vuln_type
                    )
                    
                    # Calculate false positive likelihood
                    fp_likelihood = self._estimate_false_positive_likelihood(
                        confidence, validation_score, pattern_evidence, context
                    )
                    
                    # Determine if detection meets threshold
                    risk_level = self._determine_risk_level(confidence, validation_score, context)
                    
                    if confidence >= self.confidence_thresholds[risk_level]:
                        # Extract exploit requirements and mitigation factors
                        exploit_reqs = self._identify_exploit_requirements(instructions, vuln_type, context)
                        mitigations = self._identify_mitigation_factors(instructions, context)
                        
                        detection = ImprovedVulnerabilityDetection(
                            assembly_file=assembly_file,
                            vulnerability_type=vuln_type.upper(),
                            confidence=confidence,
                            validation_score=validation_score,
                            evidence={
                                'statistical_features': statistical_features,
                                'pattern_features': pattern_features,
                                'pattern_evidence': pattern_evidence,
                                'context': asdict(context),
                                'ml_prediction': confidence
                            },
                            location_start=0,  # Could be enhanced to find specific locations
                            location_end=len(instructions),
                            risk_level=risk_level,
                            false_positive_likelihood=fp_likelihood,
                            exploit_requirements=exploit_reqs,
                            mitigation_factors=mitigations
                        )
                        
                        detections.append(detection)
                        self.logger.info(f"Detected {vuln_type} in {assembly_file} "
                                       f"(confidence: {confidence:.3f}, validation: {validation_score:.3f})")
                
                except Exception as e:
                    self.logger.warning(f"Error testing {vuln_type} on {assembly_file}: {e}")
            
            # Remove duplicate/overlapping detections
            detections = self._deduplicate_detections(detections)
            
        except Exception as e:
            self.logger.error(f"Error analyzing {assembly_file}: {e}")
        
        return detections
    
    def _is_obviously_safe(self, instructions: List[Dict], context: CodeContext) -> bool:
        """Pre-filter obviously safe code to reduce false positives"""
        
        # Skip very short code snippets
        if len(instructions) < 5:
            return True
        
        # Skip pure mathematical computations
        math_instructions = ['add', 'sub', 'mul', 'div', 'fmul', 'fadd', 'fsub', 'fdiv']
        math_count = sum(1 for instr in instructions 
                        if instr.get('mnemonic', '').lower() in math_instructions)
        
        if math_count / len(instructions) > 0.7:  # >70% math operations
            return True
        
        # Skip simple register operations
        simple_ops = ['mov', 'nop', 'ret']
        simple_count = sum(1 for instr in instructions 
                          if instr.get('mnemonic', '').lower() in simple_ops)
        
        if simple_count / len(instructions) > 0.8:  # >80% simple operations
            return True
        
        # Skip code with strong security indicators
        if (context.has_bounds_checks and context.has_speculation_barriers and 
            context.memory_safety_level == 'SAFE'):
            return True
        
        return False
    
    def _calculate_validation_score(self, ml_confidence: float, pattern_evidence: Dict,
                                  context: CodeContext, vuln_type: str) -> float:
        """Calculate composite validation score"""
        score = 0.0
        
        # ML prediction weight (40%)
        score += ml_confidence * 0.4
        
        # Pattern evidence weight (30%)
        pattern_score = len(pattern_evidence.get('matches', [])) / 5.0  # Normalize by expected patterns
        score += min(pattern_score, 1.0) * 0.3
        
        # Context weight (20%)
        context_score = 0.0
        if context.is_security_critical:
            context_score += 0.3
        if context.handles_user_input:
            context_score += 0.3
        if context.is_public_api:
            context_score += 0.2
        if not context.has_bounds_checks:
            context_score += 0.2
        
        score += min(context_score, 1.0) * 0.2
        
        # Vulnerability-specific adjustments (10%)
        vuln_adjustments = {
            'SPECTRE_V1': 0.1 if not context.has_bounds_checks else -0.1,
            'SPECTRE_V2': 0.1 if pattern_evidence.get('indirect_branches', 0) > 0 else -0.1,
            'L1TF': 0.1 if context.is_security_critical else -0.2,
            'BHI': 0.05,  # Generally lower confidence
            'MDS': 0.1 if pattern_evidence.get('memory_operations', 0) > 3 else -0.1
        }
        
        adjustment = vuln_adjustments.get(vuln_type, 0.0)
        score += adjustment * 0.1
        
        return max(0.0, min(1.0, score))
    
    def _estimate_false_positive_likelihood(self, ml_confidence: float, validation_score: float,
                                          pattern_evidence: Dict, context: CodeContext) -> float:
        """Estimate the likelihood this is a false positive"""
        
        # Start with inverse of validation score
        fp_likelihood = 1.0 - validation_score
        
        # Adjust based on red flags
        red_flags = 0
        
        # Low pattern matches
        if len(pattern_evidence.get('matches', [])) == 0:
            red_flags += 1
        
        # Safe context
        if not context.is_security_critical and not context.handles_user_input:
            red_flags += 1
        
        # Strong mitigations present
        if context.has_bounds_checks and context.has_speculation_barriers:
            red_flags += 1
        
        # Low ML confidence
        if ml_confidence < 0.6:
            red_flags += 1
        
        # Mathematical/safe code patterns
        if context.memory_safety_level == 'SAFE':
            red_flags += 1
        
        # Increase FP likelihood based on red flags
        fp_likelihood += red_flags * 0.15
        
        return max(0.0, min(1.0, fp_likelihood))
    
    def _determine_risk_level(self, confidence: float, validation_score: float, 
                            context: CodeContext) -> str:
        """Determine risk level based on multiple factors"""
        
        # Combine confidence and validation
        combined_score = (confidence + validation_score) / 2
        
        # Context-based adjustments
        if context.is_security_critical and context.handles_user_input:
            combined_score += 0.1
        
        if context.has_bounds_checks and context.has_speculation_barriers:
            combined_score -= 0.2
        
        # Determine level
        if combined_score >= 0.85:
            return 'CRITICAL'
        elif combined_score >= 0.75:
            return 'HIGH'
        elif combined_score >= 0.65:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _identify_exploit_requirements(self, instructions: List[Dict], 
                                     vuln_type: str, context: CodeContext) -> List[str]:
        """Identify what an attacker would need to exploit this vulnerability"""
        requirements = []
        
        if vuln_type == 'SPECTRE_V1':
            requirements.extend([
                "Ability to influence array index",
                "Access to probe array for side channel",
                "CPU with speculative execution"
            ])
            if not context.handles_user_input:
                requirements.append("Way to control input to function")
        
        elif vuln_type == 'SPECTRE_V2':
            requirements.extend([
                "Ability to influence indirect branch target",
                "CPU with branch prediction",
                "Method to pollute branch target buffer"
            ])
        
        elif vuln_type == 'L1TF':
            requirements.extend([
                "Kernel or hypervisor access",
                "Ability to trigger page faults",
                "Intel CPU vulnerable to L1TF"
            ])
        
        elif vuln_type == 'BHI':
            requirements.extend([
                "Ability to influence branch history",
                "Complex branch patterns in victim code",
                "CPU with branch history injection vulnerability"
            ])
        
        # Add context-specific requirements
        if not context.is_public_api:
            requirements.append("Access to internal function")
        
        if context.has_bounds_checks:
            requirements.append("Method to bypass bounds checking")
        
        return requirements
    
    def _identify_mitigation_factors(self, instructions: List[Dict], 
                                   context: CodeContext) -> List[str]:
        """Identify factors that reduce exploitability"""
        mitigations = []
        
        if context.has_bounds_checks:
            mitigations.append("Explicit bounds checking present")
        
        if context.has_speculation_barriers:
            mitigations.append("Speculation barriers present")
        
        if context.memory_safety_level == 'SAFE':
            mitigations.append("Memory-safe code patterns")
        
        # Check for specific mitigation patterns
        barrier_count = sum(1 for instr in instructions 
                          if instr.get('mnemonic', '').lower() in ['dsb', 'isb', 'lfence'])
        if barrier_count > 0:
            mitigations.append(f"{barrier_count} speculation barriers found")
        
        # Compiler optimizations can help
        if context.compiler_optimizations in ['O2', 'O3']:
            mitigations.append("High compiler optimization level")
        
        return mitigations
    
    def _deduplicate_detections(self, detections: List[ImprovedVulnerabilityDetection]) -> List[ImprovedVulnerabilityDetection]:
        """Remove duplicate or overlapping detections"""
        if not detections:
            return detections
        
        # Sort by validation score (highest first)
        detections.sort(key=lambda d: d.validation_score, reverse=True)
        
        # Remove duplicates and low-confidence overlaps
        filtered = []
        for detection in detections:
            # Check for significant overlap with existing detections
            is_duplicate = False
            for existing in filtered:
                if (detection.vulnerability_type == existing.vulnerability_type and
                    detection.assembly_file == existing.assembly_file):
                    
                    # If new detection is significantly better, replace
                    if detection.validation_score > existing.validation_score + 0.1:
                        filtered.remove(existing)
                        break
                    else:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                filtered.append(detection)
        
        return filtered
    
    def _load_enhanced_patterns(self) -> Dict[str, Any]:
        """Load enhanced vulnerability patterns"""
        # This would be loaded from a configuration file
        return {
            'SPECTRE_V1': {
                'required_patterns': ['bounds_check', 'array_access', 'speculation_window'],
                'indicators': ['cmp.*j[lg]', 'ldr.*\[.*\]', 'cache_probe'],
                'anti_patterns': ['dsb', 'isb', 'explicit_bounds_check']
            },
            'SPECTRE_V2': {
                'required_patterns': ['indirect_branch', 'branch_prediction'],
                'indicators': ['br.*x', 'blr', 'ret'],
                'anti_patterns': ['retpoline', 'lfence']
            }
            # ... more patterns
        }
    
    def _save_improved_models(self):
        """Save trained models"""
        model_data = {
            'classifiers': self.classifiers,
            'anomaly_detector': self.anomaly_detector,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'scaler': self.scaler,
            'confidence_thresholds': self.confidence_thresholds
        }
        
        with open('improved_vulnerability_models.pkl', 'wb') as f:
            pickle.dump(model_data, f)
        
        self.logger.info("Improved models saved to improved_vulnerability_models.pkl")
    
    def _validate_training_performance(self, X_features: np.ndarray, X_tfidf, 
                                     y: np.ndarray, signatures: List) -> None:
        """Validate training performance"""
        self.logger.info("Validating training performance...")
        
        # Calculate accuracy for each vulnerability type
        for vuln_type, classifier in self.classifiers.items():
            try:
                X_combined = np.hstack([X_features, X_tfidf.toarray()])
                predictions = classifier.predict(X_combined)
                
                # Calculate metrics for this vulnerability type
                vuln_mask = np.array([sig.vulnerability_type.lower() == vuln_type.lower() 
                                    for sig in signatures[:len(y)//2]])  # Only positive samples
                
                if vuln_mask.sum() > 0:
                    accuracy = np.mean(predictions[vuln_mask] == 1)
                    self.logger.info(f"{vuln_type} classifier accuracy: {accuracy:.3f}")
            
            except Exception as e:
                self.logger.warning(f"Could not validate {vuln_type} classifier: {e}")

# Import from existing modules
from robust_vulnerability_detector import VulnerabilitySignature

class CodeContextAnalyzer:
    """Analyzes code context to reduce false positives"""
    
    def analyze_context(self, assembly_file: str, instructions: List[Dict]) -> CodeContext:
        """Analyze contextual information about the code"""
        
        # Extract function name from assembly
        function_name = self._extract_function_name(assembly_file, instructions)
        
        # Analyze API characteristics
        is_public_api = self._is_public_api(function_name)
        handles_user_input = self._handles_user_input(function_name, instructions)
        is_security_critical = self._is_security_critical(assembly_file, function_name)
        
        # Analyze security features
        has_bounds_checks = self._detect_bounds_checks(instructions)
        has_speculation_barriers = self._detect_speculation_barriers(instructions)
        
        # Determine memory safety level
        memory_safety_level = self._assess_memory_safety(instructions)
        
        # Extract compiler optimization level from filename
        compiler_optimizations = self._extract_optimization_level(assembly_file)
        
        # Determine architecture
        architecture = self._determine_architecture(instructions)
        
        return CodeContext(
            function_name=function_name,
            is_public_api=is_public_api,
            handles_user_input=handles_user_input,
            is_security_critical=is_security_critical,
            has_bounds_checks=has_bounds_checks,
            has_speculation_barriers=has_speculation_barriers,
            memory_safety_level=memory_safety_level,
            compiler_optimizations=compiler_optimizations,
            architecture=architecture
        )
    
    def _extract_function_name(self, assembly_file: str, instructions: List[Dict]) -> str:
        """Extract function name from assembly file"""
        # Try to extract from filename first
        filename = Path(assembly_file).stem
        if '.' in filename:
            parts = filename.split('.')
            if len(parts) > 0:
                return parts[0]
        
        # Try to extract from assembly content (function labels)
        # This is a simplified approach
        return filename if filename else "unknown"
    
    def _is_public_api(self, function_name: str) -> bool:
        """Determine if function is likely a public API"""
        public_indicators = [
            'main', 'init', 'open', 'read', 'write', 'send', 'recv',
            'get', 'set', 'create', 'delete', 'parse', 'process'
        ]
        
        func_lower = function_name.lower()
        return any(indicator in func_lower for indicator in public_indicators)
    
    def _handles_user_input(self, function_name: str, instructions: List[Dict]) -> bool:
        """Determine if function handles user input"""
        input_indicators = [
            'input', 'parse', 'read', 'recv', 'get', 'copy', 'scanf', 'gets'
        ]
        
        func_lower = function_name.lower()
        if any(indicator in func_lower for indicator in input_indicators):
            return True
        
        # Check for input-related instructions
        for instr in instructions:
            op_str = instr.get('op_str', '').lower()
            if any(indicator in op_str for indicator in input_indicators):
                return True
        
        return False
    
    def _is_security_critical(self, assembly_file: str, function_name: str) -> bool:
        """Determine if code is security-critical"""
        security_keywords = [
            'crypto', 'auth', 'password', 'key', 'cert', 'ssl', 'tls',
            'security', 'privilege', 'admin', 'root', 'kernel'
        ]
        
        file_path = assembly_file.lower()
        func_lower = function_name.lower()
        
        return any(keyword in file_path or keyword in func_lower 
                  for keyword in security_keywords)
    
    def _detect_bounds_checks(self, instructions: List[Dict]) -> bool:
        """Detect bounds checking patterns"""
        for i in range(len(instructions) - 1):
            current = instructions[i]
            next_instr = instructions[i + 1]
            
            # Look for compare followed by conditional branch
            if (current.get('mnemonic', '').lower() == 'cmp' and 
                next_instr.get('mnemonic', '').lower().startswith('b.')):
                return True
        
        return False
    
    def _detect_speculation_barriers(self, instructions: List[Dict]) -> bool:
        """Detect speculation barrier instructions"""
        barrier_instructions = ['dsb', 'isb', 'lfence', 'mfence', 'sfence']
        
        for instr in instructions:
            mnemonic = instr.get('mnemonic', '').lower()
            if mnemonic in barrier_instructions:
                return True
        
        return False
    
    def _assess_memory_safety(self, instructions: List[Dict]) -> str:
        """Assess memory safety level of the code"""
        unsafe_count = 0
        safe_count = 0
        total_memory_ops = 0
        
        for instr in instructions:
            mnemonic = instr.get('mnemonic', '').lower()
            op_str = instr.get('op_str', '').lower()
            
            # Count memory operations
            if any(op in mnemonic for op in ['ldr', 'str', 'mov']) and '[' in op_str:
                total_memory_ops += 1
                
                # Check for unsafe patterns
                if '!' in op_str or 'uxtw' in op_str:  # Post-increment or unsafe indexing
                    unsafe_count += 1
                else:
                    safe_count += 1
        
        if total_memory_ops == 0:
            return 'SAFE'
        
        unsafe_ratio = unsafe_count / total_memory_ops
        if unsafe_ratio > 0.3:
            return 'UNSAFE'
        elif unsafe_ratio > 0.1:
            return 'MODERATE'
        else:
            return 'SAFE'
    
    def _extract_optimization_level(self, assembly_file: str) -> str:
        """Extract compiler optimization level from filename"""
        filename = assembly_file.lower()
        if 'o3' in filename:
            return 'O3'
        elif 'o2' in filename:
            return 'O2'
        elif 'o1' in filename:
            return 'O1'
        elif 'o0' in filename:
            return 'O0'
        else:
            return 'UNKNOWN'
    
    def _determine_architecture(self, instructions: List[Dict]) -> str:
        """Determine target architecture"""
        if not instructions:
            return 'UNKNOWN'
        
        # Check for ARM64 indicators
        arm64_indicators = ['x0', 'x1', 'w0', 'w1', 'ldr', 'str', 'b.']
        x86_indicators = ['rax', 'rbx', 'mov', 'push', 'pop']
        
        all_text = ' '.join([instr.get('mnemonic', '') + ' ' + instr.get('op_str', '') 
                           for instr in instructions[:10]])  # Check first 10 instructions
        
        arm64_count = sum(1 for indicator in arm64_indicators if indicator in all_text)
        x86_count = sum(1 for indicator in x86_indicators if indicator in all_text)
        
        if arm64_count > x86_count:
            return 'ARM64'
        elif x86_count > arm64_count:
            return 'X86_64'
        else:
            return 'UNKNOWN'

def main():
    """Test the improved vulnerability scanner"""
    print(" Testing Improved Vulnerability Scanner")
    
    scanner = ImprovedVulnerabilityScanner()
    
    # Train the improved model
    if scanner.train_improved_model():
        print(" Training completed successfully")
        
        # Test on a sample file
        test_files = [
            "../c_vulns/asm_code/spectre_1_arm64.s",
            "assembly_outputs/arm64/gcc/O2/repos/vlang/v/ecdsa.arm64.gcc.O2.s"
        ]
        
        for test_file in test_files:
            if os.path.exists(test_file):
                print(f"\n Testing: {test_file}")
                detections = scanner.detect_vulnerabilities_improved(test_file)
                
                for detection in detections:
                    print(f"   {detection.vulnerability_type}: {detection.confidence:.3f} "
                          f"(validation: {detection.validation_score:.3f}, "
                          f"FP risk: {detection.false_positive_likelihood:.1%})")
                    print(f"     Requirements: {', '.join(detection.exploit_requirements[:2])}")
                    print(f"     Mitigations: {', '.join(detection.mitigation_factors[:2])}")
    else:
        print(" Training failed")

if __name__ == "__main__":
    main()