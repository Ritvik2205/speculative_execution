#!/usr/bin/env python3
"""
Ensemble Vulnerability Detector
Combines multiple detection approaches for robust vulnerability identification
"""

import os
import json
import pickle
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
import joblib

from robust_vulnerability_detector import RobustVulnerabilityDetector
from semantic_vulnerability_analyzer import SemanticVulnerabilityAnalyzer

@dataclass
class EnsembleDetection:
    """Combined detection result from multiple analyzers"""
    vuln_type: str
    confidence: float
    location: Dict[str, int]
    evidence: Dict[str, Any]
    
    # Individual detector scores
    robust_score: float
    semantic_score: float
    pattern_score: float
    anomaly_score: float
    
    # Consensus metrics
    detector_agreement: float
    evidence_strength: float
    false_positive_likelihood: float
    
    # Supporting information
    detected_by: List[str]
    primary_indicators: List[str]
    risk_assessment: str

class EnsembleVulnerabilityDetector:
    """Ensemble detector combining multiple vulnerability detection approaches"""
    
    def __init__(self):
        self.robust_detector = RobustVulnerabilityDetector()
        self.semantic_analyzer = SemanticVulnerabilityAnalyzer()
        
        # Ensemble weights (can be tuned based on validation)
        self.detector_weights = {
            'robust': 0.4,
            'semantic': 0.35,
            'pattern': 0.15,
            'anomaly': 0.1
        }
        
        # Confidence thresholds
        self.confidence_thresholds = {
            'high': 0.8,
            'medium': 0.6,
            'low': 0.4
        }
        
        self.is_trained = False
    
    def train_ensemble(self, vuln_asm_dir: str):
        """Train all components of the ensemble"""
        print("üéì Training Ensemble Vulnerability Detector...")
        
        # Train robust detector
        if os.path.exists(vuln_asm_dir):
            print("   Training robust detector...")
            signatures = self.robust_detector.analyze_vulnerable_code(vuln_asm_dir)
            self.robust_detector.vulnerability_signatures = signatures
            self.robust_detector.build_ml_classifier(signatures)
            
            print(f"   ‚úÖ Robust detector trained with {len(signatures)} signatures")
        
        # Semantic analyzer doesn't require training (rule-based)
        print("   ‚úÖ Semantic analyzer ready")
        
        self.is_trained = True
        print("üéØ Ensemble training complete!")
    
    def detect_vulnerabilities(self, target_instructions: List[Dict], 
                             architecture: str,
                             context: Dict[str, Any] = None) -> List[EnsembleDetection]:
        """Detect vulnerabilities using ensemble approach"""
        
        if not self.is_trained:
            print("‚ö†Ô∏è  Ensemble not trained. Please call train_ensemble() first.")
            return []
        
        print(f"üîç Running ensemble detection on {len(target_instructions)} instructions...")
        
        # Run all detectors
        robust_detections = self._run_robust_detection(target_instructions, architecture)
        semantic_detections = self._run_semantic_detection(target_instructions, context)
        
        # Combine and consensus
        ensemble_detections = self._combine_detections(
            robust_detections, semantic_detections, target_instructions
        )
        
        # Post-process and rank
        final_detections = self._post_process_detections(ensemble_detections)
        
        print(f"üéØ Ensemble found {len(final_detections)} high-confidence vulnerabilities")
        return final_detections
    
    def _run_robust_detection(self, instructions: List[Dict], architecture: str) -> List[Dict]:
        """Run robust detector"""
        try:
            return self.robust_detector.detect_vulnerabilities(instructions, architecture)
        except Exception as e:
            print(f"‚ö†Ô∏è  Robust detector error: {e}")
            return []
    
    def _run_semantic_detection(self, instructions: List[Dict], context: Dict = None) -> List[Dict]:
        """Run semantic analyzer"""
        try:
            semantic_context = self.semantic_analyzer.analyze_code_semantics(instructions, context)
            return self.semantic_analyzer.detect_semantic_vulnerabilities(instructions, semantic_context)
        except Exception as e:
            print(f"‚ö†Ô∏è  Semantic analyzer error: {e}")
            return []
    
    def _combine_detections(self, robust_detections: List[Dict], 
                          semantic_detections: List[Dict],
                          instructions: List[Dict]) -> List[EnsembleDetection]:
        """Combine detections from multiple sources"""
        
        # Group detections by location/type
        detection_groups = defaultdict(list)
        
        # Add robust detections
        for detection in robust_detections:
            key = self._get_detection_key(detection, 'robust')
            detection_groups[key].append(('robust', detection))
        
        # Add semantic detections
        for detection in semantic_detections:
            key = self._get_detection_key(detection, 'semantic')
            detection_groups[key].append(('semantic', detection))
        
        # Create ensemble detections
        ensemble_detections = []
        
        for key, detections in detection_groups.items():
            ensemble_detection = self._create_ensemble_detection(detections, instructions)
            if ensemble_detection:
                ensemble_detections.append(ensemble_detection)
        
        return ensemble_detections
    
    def _get_detection_key(self, detection: Dict, detector_type: str) -> str:
        """Generate key for grouping similar detections"""
        vuln_type = detection.get('vuln_type', detection.get('vulnerability_types', ['UNKNOWN'])[0])
        
        if detector_type == 'robust':
            start_idx = detection.get('start_idx', 0)
            end_idx = detection.get('end_idx', 0)
        else:  # semantic
            location = detection.get('location', {})
            start_idx = location.get('start_line', 0)
            end_idx = location.get('end_line', 0)
        
        return f"{vuln_type}_{start_idx}_{end_idx}"
    
    def _create_ensemble_detection(self, detections: List[Tuple[str, Dict]], 
                                 instructions: List[Dict]) -> Optional[EnsembleDetection]:
        """Create ensemble detection from multiple detector results"""
        
        if not detections:
            return None
        
        # Extract information from detections
        detector_types = [d[0] for d in detections]
        detection_data = [d[1] for d in detections]
        
        # Determine primary vulnerability type (most common)
        vuln_types = []
        for _, detection in detections:
            vtype = detection.get('vuln_type', detection.get('vulnerability_types', ['UNKNOWN'])[0])
            if isinstance(vtype, list):
                vtype = vtype[0] if vtype else 'UNKNOWN'
            vuln_types.append(vtype)
        
        primary_vuln_type = max(set(vuln_types), key=vuln_types.count)
        
        # Calculate individual scores
        robust_score = 0.0
        semantic_score = 0.0
        pattern_score = 0.0
        anomaly_score = 0.0
        
        for detector_type, detection in detections:
            if detector_type == 'robust':
                robust_score = max(robust_score, detection.get('primary_confidence', 0.0))
                anomaly_score = max(anomaly_score, detection.get('confidence_scores', {}).get('anomaly', 0.0))
            elif detector_type == 'semantic':
                semantic_score = max(semantic_score, detection.get('confidence', 0.0))
                pattern_score = max(pattern_score, detection.get('code_score', 0.0))
        
        # Calculate ensemble confidence
        ensemble_confidence = (
            robust_score * self.detector_weights['robust'] +
            semantic_score * self.detector_weights['semantic'] +
            pattern_score * self.detector_weights['pattern'] +
            anomaly_score * self.detector_weights['anomaly']
        )
        
        # Calculate detector agreement
        detector_agreement = len(set(detector_types)) / 2.0  # Max 2 detector types
        
        # Calculate evidence strength
        evidence_strength = self._calculate_evidence_strength(detection_data)
        
        # Calculate false positive likelihood
        fp_likelihood = self._calculate_fp_likelihood(detection_data, detector_agreement)
        
        # Determine location
        location = self._determine_location(detection_data)
        
        # Collect evidence
        combined_evidence = self._combine_evidence(detection_data)
        
        # Determine risk assessment
        risk_assessment = self._assess_risk(ensemble_confidence, detector_agreement, evidence_strength)
        
        # Extract primary indicators
        primary_indicators = self._extract_primary_indicators(detection_data)
        
        return EnsembleDetection(
            vuln_type=primary_vuln_type,
            confidence=ensemble_confidence,
            location=location,
            evidence=combined_evidence,
            robust_score=robust_score,
            semantic_score=semantic_score,
            pattern_score=pattern_score,
            anomaly_score=anomaly_score,
            detector_agreement=detector_agreement,
            evidence_strength=evidence_strength,
            false_positive_likelihood=fp_likelihood,
            detected_by=detector_types,
            primary_indicators=primary_indicators,
            risk_assessment=risk_assessment
        )
    
    def _calculate_evidence_strength(self, detection_data: List[Dict]) -> float:
        """Calculate strength of evidence across detections"""
        total_evidence = 0
        
        for detection in detection_data:
            evidence = detection.get('evidence', {})
            if isinstance(evidence, dict):
                # Count different types of evidence
                total_evidence += len(evidence.get('matching_instructions', []))
                total_evidence += len(evidence.get('semantic_indicators', []))
                total_evidence += len(evidence.get('vulnerability_patterns', []))
        
        return min(total_evidence / 10.0, 1.0)  # Normalize to 0-1
    
    def _calculate_fp_likelihood(self, detection_data: List[Dict], agreement: float) -> float:
        """Calculate likelihood of false positive"""
        base_fp = 0.3  # Base false positive rate
        
        # Lower FP likelihood with higher agreement
        agreement_factor = (1.0 - agreement) * 0.3
        
        # Check for false positive indicators
        fp_indicators = 0
        for detection in detection_data:
            if detection.get('fp_penalty', 0) > 0:
                fp_indicators += 1
        
        fp_penalty = fp_indicators * 0.2
        
        return min(base_fp + agreement_factor + fp_penalty, 1.0)
    
    def _determine_location(self, detection_data: List[Dict]) -> Dict[str, int]:
        """Determine best location estimate from multiple detections"""
        start_lines = []
        end_lines = []
        
        for detection in detection_data:
            if 'start_idx' in detection:
                start_lines.append(detection['start_idx'])
                end_lines.append(detection['end_idx'])
            elif 'location' in detection:
                loc = detection['location']
                start_lines.append(loc.get('start_line', 0))
                end_lines.append(loc.get('end_line', 0))
        
        return {
            'start_line': min(start_lines) if start_lines else 0,
            'end_line': max(end_lines) if end_lines else 0
        }
    
    def _combine_evidence(self, detection_data: List[Dict]) -> Dict[str, Any]:
        """Combine evidence from multiple detections"""
        combined = {
            'detector_results': detection_data,
            'matching_patterns': [],
            'semantic_indicators': [],
            'anomaly_indicators': [],
            'confidence_breakdown': {}
        }
        
        for detection in detection_data:
            # Extract patterns
            if 'vulnerability_patterns' in detection:
                combined['matching_patterns'].extend(detection['vulnerability_patterns'])
            
            # Extract semantic indicators
            evidence = detection.get('evidence', {})
            if 'semantic_indicators' in evidence:
                combined['semantic_indicators'].extend(evidence['semantic_indicators'])
            
            # Extract confidence breakdown
            if 'confidence_scores' in detection:
                combined['confidence_breakdown'].update(detection['confidence_scores'])
        
        return combined
    
    def _assess_risk(self, confidence: float, agreement: float, evidence_strength: float) -> str:
        """Assess overall risk level"""
        risk_score = (confidence * 0.5 + agreement * 0.3 + evidence_strength * 0.2)
        
        if risk_score >= 0.8:
            return "CRITICAL"
        elif risk_score >= 0.6:
            return "HIGH"
        elif risk_score >= 0.4:
            return "MEDIUM"
        else:
            return "LOW"
    
    def _extract_primary_indicators(self, detection_data: List[Dict]) -> List[str]:
        """Extract primary vulnerability indicators"""
        indicators = set()
        
        for detection in detection_data:
            # From robust detector
            if 'vulnerability_patterns' in detection:
                indicators.update(detection['vulnerability_patterns'])
            
            # From semantic analyzer
            evidence = detection.get('evidence', {})
            if 'semantic_indicators' in evidence:
                indicators.update(evidence['semantic_indicators'])
        
        return list(indicators)[:5]  # Top 5 indicators
    
    def _post_process_detections(self, detections: List[EnsembleDetection]) -> List[EnsembleDetection]:
        """Post-process and rank ensemble detections"""
        
        # Filter by minimum confidence
        filtered = [d for d in detections if d.confidence >= 0.4]
        
        # Sort by confidence and evidence strength
        filtered.sort(key=lambda x: (x.confidence, x.evidence_strength, x.detector_agreement), reverse=True)
        
        # Remove overlapping detections
        final_detections = []
        for detection in filtered:
            if not self._overlaps_with_existing(detection, final_detections):
                final_detections.append(detection)
        
        return final_detections[:10]  # Top 10 detections
    
    def _overlaps_with_existing(self, detection: EnsembleDetection, 
                               existing: List[EnsembleDetection]) -> bool:
        """Check if detection overlaps with existing ones"""
        for existing_det in existing:
            if (detection.vuln_type == existing_det.vuln_type and
                self._locations_overlap(detection.location, existing_det.location)):
                return True
        return False
    
    def _locations_overlap(self, loc1: Dict[str, int], loc2: Dict[str, int]) -> bool:
        """Check if two locations overlap significantly"""
        start1, end1 = loc1.get('start_line', 0), loc1.get('end_line', 0)
        start2, end2 = loc2.get('start_line', 0), loc2.get('end_line', 0)
        
        overlap = max(0, min(end1, end2) - max(start1, start2))
        min_length = min(end1 - start1, end2 - start2)
        
        return overlap / max(min_length, 1) > 0.5  # 50% overlap threshold
    
    def generate_report(self, detections: List[EnsembleDetection], 
                       output_file: str = "vulnerability_report.json"):
        """Generate comprehensive vulnerability report"""
        
        report = {
            'summary': {
                'total_detections': len(detections),
                'by_risk_level': self._count_by_risk(detections),
                'by_vulnerability_type': self._count_by_type(detections),
                'high_confidence_count': len([d for d in detections if d.confidence >= 0.8])
            },
            'detections': []
        }
        
        for i, detection in enumerate(detections):
            detection_report = {
                'id': i + 1,
                'vulnerability_type': detection.vuln_type,
                'risk_assessment': detection.risk_assessment,
                'confidence': round(detection.confidence, 3),
                'location': detection.location,
                'detected_by': detection.detected_by,
                'detector_scores': {
                    'robust': round(detection.robust_score, 3),
                    'semantic': round(detection.semantic_score, 3),
                    'pattern': round(detection.pattern_score, 3),
                    'anomaly': round(detection.anomaly_score, 3)
                },
                'metrics': {
                    'detector_agreement': round(detection.detector_agreement, 3),
                    'evidence_strength': round(detection.evidence_strength, 3),
                    'false_positive_likelihood': round(detection.false_positive_likelihood, 3)
                },
                'primary_indicators': detection.primary_indicators,
                'evidence_summary': self._summarize_evidence(detection.evidence)
            }
            report['detections'].append(detection_report)
        
        # Save report
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"üìä Vulnerability report saved to {output_file}")
        return report
    
    def _count_by_risk(self, detections: List[EnsembleDetection]) -> Dict[str, int]:
        """Count detections by risk level"""
        counts = {'CRITICAL': 0, 'HIGH': 0, 'MEDIUM': 0, 'LOW': 0}
        for detection in detections:
            counts[detection.risk_assessment] += 1
        return counts
    
    def _count_by_type(self, detections: List[EnsembleDetection]) -> Dict[str, int]:
        """Count detections by vulnerability type"""
        counts = defaultdict(int)
        for detection in detections:
            counts[detection.vuln_type] += 1
        return dict(counts)
    
    def _summarize_evidence(self, evidence: Dict[str, Any]) -> Dict[str, Any]:
        """Summarize evidence for report"""
        return {
            'pattern_matches': len(evidence.get('matching_patterns', [])),
            'semantic_indicators': len(evidence.get('semantic_indicators', [])),
            'detector_count': len(evidence.get('detector_results', [])),
            'top_indicators': evidence.get('semantic_indicators', [])[:3]
        }
    
    def save_ensemble_model(self, filepath: str):
        """Save the entire ensemble model"""
        try:
            # Create a directory for the ensemble model if it doesn't exist
            model_dir = filepath.replace('.pkl', '_ensemble')
            os.makedirs(model_dir, exist_ok=True)
            
            # Save the robust detector components separately to avoid ctypes issues
            print("üíæ Saving robust detector components...")
            
            # Save ML models individually using joblib
            if hasattr(self.robust_detector, 'ml_classifier') and self.robust_detector.ml_classifier is not None:
                ml_classifier_path = os.path.join(model_dir, 'ml_classifier.joblib')
                joblib.dump(self.robust_detector.ml_classifier, ml_classifier_path)
                print(f"   ‚úÖ ML classifier saved: {ml_classifier_path}")
            
            if hasattr(self.robust_detector, 'anomaly_detector') and self.robust_detector.anomaly_detector is not None:
                anomaly_detector_path = os.path.join(model_dir, 'anomaly_detector.joblib')
                joblib.dump(self.robust_detector.anomaly_detector, anomaly_detector_path)
                print(f"   ‚úÖ Anomaly detector saved: {anomaly_detector_path}")
            
            if hasattr(self.robust_detector, 'scaler') and self.robust_detector.scaler is not None:
                scaler_path = os.path.join(model_dir, 'scaler.joblib')
                joblib.dump(self.robust_detector.scaler, scaler_path)
                print(f"   ‚úÖ Scaler saved: {scaler_path}")
            
            # Save non-ML attributes of robust detector using pickle
            robust_detector_data = {}
            for attr in ['vulnerability_signatures', 'signature_patterns', 'pattern_weights']:
                if hasattr(self.robust_detector, attr):
                    robust_detector_data[attr] = getattr(self.robust_detector, attr)
            
            robust_data_path = os.path.join(model_dir, 'robust_detector_data.pkl')
            with open(robust_data_path, 'wb') as f:
                pickle.dump(robust_detector_data, f)
            print(f"   ‚úÖ Robust detector data saved: {robust_data_path}")
            
            # Save ensemble configuration using pickle
            ensemble_data = {
                'detector_weights': self.detector_weights,
                'confidence_thresholds': self.confidence_thresholds,
                'is_trained': self.is_trained
            }
            
            ensemble_config_path = os.path.join(model_dir, 'ensemble_config.pkl')
            with open(ensemble_config_path, 'wb') as f:
                pickle.dump(ensemble_data, f)
            print(f"   ‚úÖ Ensemble configuration saved: {ensemble_config_path}")
            
            # Save metadata about the saved model
            metadata = {
                'model_type': 'ensemble_vulnerability_detector',
                'version': '2.0',
                'components': ['ml_classifier', 'anomaly_detector', 'scaler', 'robust_detector_data', 'ensemble_config'],
                'saved_at': os.getcwd(),
                'save_method': 'granular_component_saving'
            }
            
            metadata_path = os.path.join(model_dir, 'metadata.json')
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"   ‚úÖ Metadata saved: {metadata_path}")
            
            print(f"üíæ Ensemble model successfully saved to {model_dir}")
            
        except Exception as e:
            print(f"‚ùå Error saving ensemble model: {e}")
            # Fallback: try to save just the configuration without the ML models
            try:
                fallback_data = {
                    'detector_weights': self.detector_weights,
                    'confidence_thresholds': self.confidence_thresholds,
                    'is_trained': False  # Mark as not trained since we can't save the models
                }
                fallback_path = filepath.replace('.pkl', '_config_only.pkl')
                with open(fallback_path, 'wb') as f:
                    pickle.dump(fallback_data, f)
                print(f"‚ö†Ô∏è  Saved configuration only to {fallback_path}")
                print("   Note: You'll need to retrain the ensemble after loading")
            except Exception as fallback_error:
                print(f"‚ùå Even fallback save failed: {fallback_error}")
                raise
    
    def load_ensemble_model(self, filepath: str):
        """Load a trained ensemble model"""
        try:
            # Determine if this is the new format (directory) or old format (single file)
            model_dir = filepath.replace('.pkl', '_ensemble')
            
            if os.path.isdir(model_dir):
                print(f"üìÇ Loading ensemble model from {model_dir}")
                
                # Check metadata to determine loading method
                metadata_path = os.path.join(model_dir, 'metadata.json')
                if os.path.exists(metadata_path):
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    print(f"   üìã Model info: {metadata.get('model_type', 'unknown')} v{metadata.get('version', 'unknown')}")
                    save_method = metadata.get('save_method', 'legacy')
                else:
                    save_method = 'legacy'
                
                if save_method == 'granular_component_saving':
                    # New granular format: reconstruct robust detector from components
                    print("   üîß Reconstructing robust detector from components...")
                    self.robust_detector = RobustVulnerabilityDetector()
                    
                    # Load ML components individually
                    ml_classifier_path = os.path.join(model_dir, 'ml_classifier.joblib')
                    if os.path.exists(ml_classifier_path):
                        self.robust_detector.ml_classifier = joblib.load(ml_classifier_path)
                        print(f"   ‚úÖ ML classifier loaded: {ml_classifier_path}")
                    
                    anomaly_detector_path = os.path.join(model_dir, 'anomaly_detector.joblib')
                    if os.path.exists(anomaly_detector_path):
                        self.robust_detector.anomaly_detector = joblib.load(anomaly_detector_path)
                        print(f"   ‚úÖ Anomaly detector loaded: {anomaly_detector_path}")
                    
                    scaler_path = os.path.join(model_dir, 'scaler.joblib')
                    if os.path.exists(scaler_path):
                        self.robust_detector.scaler = joblib.load(scaler_path)
                        print(f"   ‚úÖ Scaler loaded: {scaler_path}")
                    
                    # Load non-ML data
                    robust_data_path = os.path.join(model_dir, 'robust_detector_data.pkl')
                    if os.path.exists(robust_data_path):
                        with open(robust_data_path, 'rb') as f:
                            robust_data = pickle.load(f)
                        
                        for attr, value in robust_data.items():
                            setattr(self.robust_detector, attr, value)
                        print(f"   ‚úÖ Robust detector data loaded: {robust_data_path}")
                    
                    # Load ensemble configuration
                    ensemble_config_path = os.path.join(model_dir, 'ensemble_config.pkl')
                    if os.path.exists(ensemble_config_path):
                        with open(ensemble_config_path, 'rb') as f:
                            config_data = pickle.load(f)
                        
                        self.detector_weights = config_data['detector_weights']
                        self.confidence_thresholds = config_data['confidence_thresholds']
                        self.is_trained = config_data['is_trained']
                        print(f"   ‚úÖ Ensemble configuration loaded: {ensemble_config_path}")
                
                else:
                    # Legacy directory format
                    robust_detector_path = os.path.join(model_dir, 'robust_detector.joblib')
                    if os.path.exists(robust_detector_path):
                        self.robust_detector = joblib.load(robust_detector_path)
                        print(f"   ‚úÖ Robust detector loaded from {robust_detector_path}")
                    else:
                        print(f"   ‚ö†Ô∏è  Robust detector not found, creating new instance")
                        self.robust_detector = RobustVulnerabilityDetector()
                    
                    # Load other configuration
                    other_data_path = os.path.join(model_dir, 'ensemble_config.pkl')
                    if os.path.exists(other_data_path):
                        with open(other_data_path, 'rb') as f:
                            config_data = pickle.load(f)
                        
                        self.detector_weights = config_data['detector_weights']
                        self.confidence_thresholds = config_data['confidence_thresholds']
                        self.is_trained = config_data['is_trained']
                        print(f"   ‚úÖ Configuration loaded from {other_data_path}")
                    else:
                        print(f"   ‚ö†Ô∏è  Configuration not found, using defaults")
                
            elif filepath.endswith('_config_only.pkl') and os.path.exists(filepath):
                # Config-only format (fallback save)
                print(f"üìÇ Loading configuration-only model from {filepath}")
                with open(filepath, 'rb') as f:
                    config_data = pickle.load(f)
                
                self.detector_weights = config_data['detector_weights']
                self.confidence_thresholds = config_data['confidence_thresholds']
                self.is_trained = config_data['is_trained']
                
                # Create new detector instances
                self.robust_detector = RobustVulnerabilityDetector()
                print("   ‚ö†Ô∏è  Configuration loaded, but you'll need to retrain the models")
                
            elif os.path.exists(filepath):
                # Old format: try to load single file (might fail with ctypes error)
                print(f"üìÇ Attempting to load legacy model from {filepath}")
                with open(filepath, 'rb') as f:
                    model_data = pickle.load(f)
                
                self.robust_detector = model_data['robust_detector']
                self.detector_weights = model_data['detector_weights']
                self.confidence_thresholds = model_data['confidence_thresholds']
                self.is_trained = model_data['is_trained']
                print(f"   ‚úÖ Legacy model loaded successfully")
                
            else:
                raise FileNotFoundError(f"No ensemble model found at {filepath} or {model_dir}")
            
            print(f"üéØ Ensemble model loading complete!")
            
        except Exception as e:
            print(f"‚ùå Error loading ensemble model: {e}")
            print("   Creating fresh ensemble instance...")
            self.__init__()  # Reset to fresh state
            raise

def main():
    """Test the ensemble vulnerability detector"""
    print("üéØ Testing Ensemble Vulnerability Detector")
    
    # Initialize ensemble
    ensemble = EnsembleVulnerabilityDetector()
    
    # Train ensemble
    vuln_asm_dir = "../c_vulns/asm_code"
    if os.path.exists(vuln_asm_dir):
        ensemble.train_ensemble(vuln_asm_dir)
        
        # Test with sample vulnerable code
        test_instructions = [
            {
                'line_num': 1,
                'raw_line': 'cmp $0x100, %rax',
                'opcode': 'cmp',
                'operands': ['$0x100', '%rax'],
                'semantics': {'is_comparison': True, 'is_branch': False, 'accesses_memory': False}
            },
            {
                'line_num': 2,
                'raw_line': 'jae .L_safe',
                'opcode': 'jae',
                'operands': ['.L_safe'],
                'semantics': {'is_branch': True, 'is_conditional': True, 'accesses_memory': False}
            },
            {
                'line_num': 3,
                'raw_line': 'movzbl (%rsi,%rax,1), %edx',
                'opcode': 'movzbl',
                'operands': ['(%rsi,%rax,1)', '%edx'],
                'semantics': {'is_load': True, 'accesses_memory': True, 'is_branch': False}
            },
            {
                'line_num': 4,
                'raw_line': 'mov %edx, %eax',
                'opcode': 'mov',
                'operands': ['%edx', '%eax'],
                'semantics': {'is_load': False, 'accesses_memory': False, 'is_branch': False}
            },
            {
                'line_num': 5,
                'raw_line': 'shl $6, %rax',
                'opcode': 'shl',
                'operands': ['$6', '%rax'],
                'semantics': {'is_arithmetic': True, 'accesses_memory': False, 'is_branch': False}
            },
            {
                'line_num': 6,
                'raw_line': 'mov probe_array(%rax), %cl',
                'opcode': 'mov',
                'operands': ['probe_array(%rax)', '%cl'],
                'semantics': {'is_load': True, 'accesses_memory': True, 'is_branch': False}
            }
        ]
        
        # Run ensemble detection
        detections = ensemble.detect_vulnerabilities(test_instructions, 'x86_64')
        
        print(f"\nüéØ Ensemble Detection Results:")
        for i, detection in enumerate(detections):
            print(f"\n   Detection {i+1}:")
            print(f"      Type: {detection.vuln_type}")
            print(f"      Risk: {detection.risk_assessment}")
            print(f"      Confidence: {detection.confidence:.3f}")
            print(f"      Detected by: {', '.join(detection.detected_by)}")
            print(f"      Agreement: {detection.detector_agreement:.3f}")
            print(f"      Evidence strength: {detection.evidence_strength:.3f}")
            print(f"      Primary indicators: {', '.join(detection.primary_indicators[:3])}")
        
        # Generate report
        if detections:
            report = ensemble.generate_report(detections, "test_vulnerability_report.json")
            print(f"\nüìä Report Summary:")
            print(f"   Total detections: {report['summary']['total_detections']}")
            print(f"   By risk: {report['summary']['by_risk_level']}")
            print(f"   By type: {report['summary']['by_vulnerability_type']}")
        
        # Save ensemble model
        ensemble.save_ensemble_model("ensemble_vulnerability_model.pkl")
        
    else:
        print(f"‚ùå Vulnerable assembly directory not found: {vuln_asm_dir}")

if __name__ == "__main__":
    main()