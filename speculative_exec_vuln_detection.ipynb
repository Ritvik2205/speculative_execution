
# Speculative Execution Vulnerability Detection Pipeline

This notebook provides a starter pipeline for detecting speculative execution vulnerabilities across code samples and hardware platforms.

---

## ðŸ§© Step 1: Setup

Install required libraries and define basic imports.



# !pip install pandas numpy scikit-learn networkx torch matplotlib seaborn

import os
import re
import json
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler



## ðŸ§ª Step 2: Load and Parse Code Samples

Assume code samples are in `data/` with subfolders: `vulnerable/` and `safe/`.

Each file is a `.c` or `.cpp` file.

We'll extract basic static features for now (e.g., presence of bounds checks, indirect branches).




def load_code_samples(path):
    data = []
    for label in ['vulnerable', 'safe']:
        folder = os.path.join(path, label)
        for file in os.listdir(folder):
            if file.endswith('.c') or file.endswith('.cpp'):
                with open(os.path.join(folder, file), 'r') as f:
                    code = f.read()
                    data.append({'code': code, 'label': 1 if label == 'vulnerable' else 0})
    return pd.DataFrame(data)

df = load_code_samples('data')
df.head()



def extract_static_features(code):
    features = {
        'has_bounds_check': int(bool(re.search(r'if\s*\(.*<.*\)', code))),
        'has_array_access': int(bool(re.search(r'\[.*\]', code))),
        'has_indirect_branch': int(bool(re.search(r'\*\w+\(', code))),
        'code_length': len(code),
        'num_if': code.count('if'),
        'num_for': code.count('for'),
        'num_while': code.count('while')
    }
    return features

features = df['code'].apply(extract_static_features)
X = pd.DataFrame(features.tolist())
y = df['label']



## ðŸ¤– Step 3: Train a Simple Classifier

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(classification_report(y_test, y_pred))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

